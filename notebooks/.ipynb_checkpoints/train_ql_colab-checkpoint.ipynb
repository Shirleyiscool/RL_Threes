{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T03:52:49.735795Z",
     "start_time": "2020-06-25T03:52:49.682624Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "HCTO0iNG4n9V"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def movable_condition(first, second):\n",
    "    \"\"\"Define whether two close tile can be merged\"\"\"\n",
    "    return ((first == 0) and (second != 0)) or \\\n",
    "           ((np.any(np.array([first, second]) > 2)) and (first == second)) or \\\n",
    "           ((first, second) == (1, 2)) or \\\n",
    "           ((second, first) == (1, 2))\n",
    "\n",
    "\n",
    "def can_move_col(array):\n",
    "    \"\"\"Check whether an array can be merged.\"\"\"\n",
    "    for i in range(3):\n",
    "        first, second = array[i], array[i + 1]\n",
    "        if movable_condition(first, second):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def allowed_moves(state):\n",
    "    \"\"\"Find allowed moves for a certain game state\"\"\"\n",
    "    allowed_actions = []\n",
    "    # Check whether the agent can swipe up\n",
    "    if np.any([can_move_col(col) for col in state.T]):\n",
    "        allowed_actions.append('w')\n",
    "    # Check whether the agent can swipe down\n",
    "    if np.any([can_move_col(col[::-1]) for col in state.T]):\n",
    "        allowed_actions.append('s')\n",
    "    # Check whether the agent can swipe left\n",
    "    if np.any([can_move_col(row) for row in state]):\n",
    "        allowed_actions.append('a')\n",
    "    # Check whether the agent can swipe right\n",
    "    if np.any([can_move_col(row[::-1]) for row in state]):\n",
    "        allowed_actions.append('d')\n",
    "    return allowed_actions\n",
    "\n",
    "\n",
    "def try_move_col(array):\n",
    "    \"\"\"Return the next state for an array\"\"\"\n",
    "    new_array = array.copy()\n",
    "    for i in range(3):\n",
    "        first, second = array[i], array[i + 1]\n",
    "        if movable_condition(first, second):\n",
    "            new_array[i] = first + second\n",
    "            new_array[i + 1:] = np.append(new_array[i + 2:], 0)\n",
    "            return new_array\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "\n",
    "def get_reward(current_state, next_state):\n",
    "    \"\"\"Given the current state and the next state, return the reward for the transition action.\"\"\"\n",
    "    reward = 0\n",
    "    # maximum number gets larger\n",
    "    reward += (np.max(next_state) - np.max(current_state))\n",
    "    # more merge\n",
    "    reward += (get_score(next_state) - get_score(current_state))\n",
    "    return reward\n",
    "\n",
    "\n",
    "def get_score(state):\n",
    "    \"\"\"Get score given a game state\"\"\"\n",
    "    all_power = [3**(np.log2(num/3)+1)\n",
    "                 for row in state for num in row if num > 3]\n",
    "    return np.sum(all_power)\n",
    "\n",
    "\n",
    "def try_move(current_state, action):\n",
    "    \"\"\"Given the state and the chosen action, return the next state\"\"\"\n",
    "    next_state = current_state.copy()\n",
    "    allowed_actions = allowed_moves(current_state)\n",
    "    if action not in allowed_actions:\n",
    "        print(f'Can not move {action}')\n",
    "        return current_state\n",
    "\n",
    "    # Swipe up\n",
    "    if action == 'w':\n",
    "        for i, col in enumerate(current_state.T):\n",
    "            if can_move_col(col):\n",
    "                next_state.T[i] = try_move_col(col)\n",
    "    # Swipe down\n",
    "    elif action == 's':\n",
    "        for i, col in enumerate(current_state.T):\n",
    "            if can_move_col(col[::-1]):\n",
    "                new_array = try_move_col(col[::-1])\n",
    "                next_state.T[i] = new_array[::-1]\n",
    "    # Swipe left\n",
    "    elif action == 'a':\n",
    "        for i, col in enumerate(current_state):\n",
    "            if can_move_col(col):\n",
    "                next_state[i] = try_move_col(col)\n",
    "    # Swipe right\n",
    "    elif action == 'd':\n",
    "        for i, col in enumerate(current_state):\n",
    "            if can_move_col(col[::-1]):\n",
    "                new_array = try_move_col(col[::-1])\n",
    "                next_state[i] = new_array[::-1]\n",
    "\n",
    "    elif action == 'stop':\n",
    "        return current_state, get_score(current_state)\n",
    "\n",
    "    reward = get_reward(current_state, next_state)\n",
    "\n",
    "    return next_state, reward\n",
    "\n",
    "\n",
    "class Threes:\n",
    "    \"\"\"\n",
    "    This is a simulated environment of game Threes.\n",
    "    Swipe direction: {left: 'a', right: 'd', up: 'w', down: 's'}.\n",
    "    There are two levels for this game: ['hard', 'easy'], in which default level is 'hard'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, level='hard'):\n",
    "        \"\"\"Initialize the game\"\"\"\n",
    "        self.state = np.zeros((4, 4))\n",
    "        x, y = np.random.choice(4, 2)\n",
    "        self.state[x, y] = np.random.choice([1, 2])\n",
    "        self.score = get_score(self.state)\n",
    "        self.level = level\n",
    "\n",
    "    def playable(self):\n",
    "        \"\"\"Check whether the game is still playable.\"\"\"\n",
    "        if len(allowed_moves(self.state)) != 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def gen_new_tile(self):\n",
    "        \"\"\"Generate a new tile after each move.\"\"\"\n",
    "\n",
    "        # Basic list of numbers that can be selected\n",
    "        choice_list = [1, 2, 3]\n",
    "\n",
    "        # More number can be selected when the maximum number on the grid gets larger\n",
    "        if np.max(self.state) % 3 == 0:\n",
    "            max_power = np.int(np.log2(np.max(self.state) / 3))\n",
    "            choice_list += [3 * 2 ** i for i in range(max_power + 1)]\n",
    "\n",
    "        # Generate the probabilities for each candidate\n",
    "        if self.level == 'hard':\n",
    "            norm_prob = [1 / len(choice_list)] * len(choice_list)\n",
    "        else:\n",
    "            prob = [i + 1 for i in range(len(choice_list))][::-1]\n",
    "            norm_prob = [num / sum(prob) for num in prob]\n",
    "\n",
    "            # return next number\n",
    "        return np.random.choice(choice_list, p=norm_prob)\n",
    "\n",
    "    def make_move(self, action):\n",
    "        \"\"\"Given the action, the game goes to the next state\"\"\"\n",
    "        if action == 'stop':\n",
    "            self.score = get_score(self.state)\n",
    "            return self.state, self.score\n",
    "\n",
    "        self.state = try_move(self.state, action)[0]\n",
    "\n",
    "        # generate new tile for the current state\n",
    "        new_tile = self.gen_new_tile()\n",
    "        loc_0 = np.argwhere(self.state == 0)\n",
    "        x, y = loc_0[np.random.choice(len(loc_0))]\n",
    "\n",
    "        # Update the game state and scores\n",
    "        self.state[x, y] = new_tile\n",
    "        self.score = get_score(self.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T03:52:49.780778Z",
     "start_time": "2020-06-25T03:52:49.745358Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "J11JYJTCbFu7"
   },
   "outputs": [],
   "source": [
    "def hashable(state):\n",
    "    \"\"\"Switch state matrix to string matrix, so as to make it hashable.\"\"\"\n",
    "    return ', '.join([str(int(i)) for row in state for i in row])\n",
    "\n",
    "\n",
    "def select_best_move_(game):\n",
    "    \"\"\"Selects best move that can get the maximum reward for the next state\"\"\"\n",
    "    possible_next_actions = allowed_moves(game.state)\n",
    "    state_action_score = [(move, try_move(game.state, move)[1])\n",
    "                          for move in possible_next_actions]\n",
    "    max_score = max(state_action_score, key=lambda item: item[1])[1]\n",
    "    max_move_list = [move for move,\n",
    "                     score in state_action_score if score == max_score]\n",
    "    best_next_move = np.random.choice(max_move_list)\n",
    "    return best_next_move\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    This is an agent to play game \"Threes\". There are two main mode to play the game. One is human mode and the other\n",
    "    is computer mode(demo game). For the computer mode, there are currently three methods to play the game: [\n",
    "    'random', 'max', 'q-learning'] The functions here are inspired by\n",
    "    \"https://github.com/brianspiering/rl-course/blob/master/labs/lab_4_tic_tac_toe/lab_4_tic_tac_toe.ipynb\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, threes, epsilon=0.1, alpha=1.0):\n",
    "        \"\"\"Initial the Agent.\"\"\"\n",
    "        self.V = dict()\n",
    "        self.NewGame = threes\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def state_value(self, game_state, action):\n",
    "        \"\"\"Look up state value. If never seen state, then assume 0.\"\"\"\n",
    "        return self.V.get((hashable(game_state), action), 0.0)\n",
    "\n",
    "    def state_values(self, game_state, actions):\n",
    "        \"\"\"Return a dictionary of state-value pair. It is for finding the action that can maximize the q value \"\"\"\n",
    "        return dict(((hashable(game_state), action), self.state_value(game_state, action)) for action in actions)\n",
    "\n",
    "    def learn_game(self, n_episodes=1000):\n",
    "        \"\"\"Let's learn through complete experience to get that reward.\"\"\"\n",
    "        for e in range(1, n_episodes + 1):\n",
    "            game = self.NewGame()\n",
    "            while game.playable():\n",
    "                action, reward = self.learn_from_move(game)\n",
    "            self.V[(hashable(game.state), action)] = reward\n",
    "\n",
    "    def learn_from_move(self, game):\n",
    "        \"\"\"The heart of Q-learning.\"\"\"\n",
    "\n",
    "        current_state = game.state\n",
    "        # Select next action with epsilon-greedy method\n",
    "        selected_move = self.learn_select_move(game)\n",
    "\n",
    "        # Next state s(t+1) and reward r\n",
    "        next_state, reward = try_move(current_state, selected_move)\n",
    "\n",
    "        # Current state Q value Q(s, a)\n",
    "        old_value = self.state_value(current_state, selected_move)\n",
    "\n",
    "        # best action a* for the next state with the largest q value Q(st+1, a*)\n",
    "        next_max_V, next_max_move = self.select_best_move(game, next_state)\n",
    "\n",
    "        # Q-learning that updates the q-value\n",
    "        self.V[(hashable(current_state), selected_move)] = (1 - self.alpha) * old_value + self.alpha * (\n",
    "            reward + next_max_V)\n",
    "\n",
    "        game.make_move(selected_move)\n",
    "        return selected_move, reward\n",
    "\n",
    "    def learn_select_move(self, game):\n",
    "        \"\"\"Exploration and exploitation\"\"\"\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            selected_action = np.random.choice(allowed_moves(game.state))\n",
    "        else:\n",
    "            selected_action = self.select_best_move(game, game.state)[1]\n",
    "        return selected_action\n",
    "\n",
    "    def select_best_move(self, game, game_state):\n",
    "        \"\"\"Selects best move for given state(Greedy)\"\"\"\n",
    "        state_action_values = self.state_values(\n",
    "            game_state, allowed_moves(game_state))\n",
    "        max_V = max(state_action_values.values())\n",
    "        max_move = np.random.choice(\n",
    "            [state_action[1] for state_action, v in state_action_values.items() if v == max_V])\n",
    "        return max_V, max_move\n",
    "\n",
    "    def demo_game(self, level='hard', mode='random'):\n",
    "        \"\"\"Agent plays with different policies (random/max/q-learning)\"\"\"\n",
    "        game = self.NewGame()\n",
    "        game.level = level\n",
    "        while game.playable():\n",
    "            if mode == 'random':\n",
    "                next_action = np.random.choice(allowed_moves(game.state))\n",
    "            elif mode == 'max':\n",
    "                next_action = select_best_move_(game)\n",
    "            elif mode == 'q-learning':\n",
    "                next_action = self.select_best_move(game, game.state)[1]\n",
    "            else:\n",
    "                return \"No such mode\"\n",
    "            game.make_move(next_action)\n",
    "        return game.score\n",
    "\n",
    "    def human_mode(self):\n",
    "        \"\"\"Interactive mode\"\"\"\n",
    "        game = self.NewGame()\n",
    "        level = input('level: easy or hard?')\n",
    "        game.level = level\n",
    "        print(game.state)\n",
    "        while game.playable():\n",
    "            human_allowed_moves = allowed_moves(game.state) + ['stop']\n",
    "            human_move = input(f'You can input {human_allowed_moves}')\n",
    "            if human_move == 'stop':\n",
    "                return f'Game over! Your score is {game.score}'\n",
    "            game.make_move(human_move)\n",
    "            print(game.state)\n",
    "        return f'Game over! Your score is {game.score}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T03:52:49.856830Z",
     "start_time": "2020-06-25T03:52:49.847293Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "NaBoR1sKbFvB"
   },
   "outputs": [],
   "source": [
    "def print_demo_game_stats(agent, n_games=100, level='Hard', mode='random'):\n",
    "    \"\"\"print the result(mean score and max score) of playing demo game\"\"\"\n",
    "    results = [agent.demo_game(level=level, mode=mode) for _ in range(n_games)]\n",
    "    median_score, max_score = np.median(results), np.max(results)\n",
    "    print(f\"mean score: {median_score} | max score: {max_score} \")\n",
    "    return median_score, max_score\n",
    "\n",
    "\n",
    "def train_qlearning(agent, n_games=100, n_episodes=100, n_training_blocks=10, level='Hard'):\n",
    "    \"\"\"Given agent, do more training. Return (hopefully) improved agent.\"\"\"\n",
    "    mean_scores = []\n",
    "    max_scores = []\n",
    "    for n_training_block in range(1, n_training_blocks + 1):\n",
    "        agent.learn_game(n_episodes)\n",
    "        print(f\"After {n_episodes * n_training_block:,} learning games:\")\n",
    "        mean_score, max_score = print_demo_game_stats(\n",
    "            agent, n_games=n_games, level=level, mode='q-learning')\n",
    "        mean_scores.append(mean_score)\n",
    "        max_scores.append(max_score)\n",
    "    return agent, mean_scores, max_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "XnvE7UuYMj4m",
    "outputId": "18648d12-32a7-4a1e-ddea-a88f3ee1188e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q-learning:\n",
      "After 500 learning games:\n",
      "mean score: 193.5 | max score: 3555.0 \n",
      "After 1,000 learning games:\n",
      "mean score: 189.0 | max score: 2619.0 \n",
      "After 1,500 learning games:\n",
      "mean score: 189.0 | max score: 3717.0 \n",
      "After 2,000 learning games:\n",
      "mean score: 189.0 | max score: 3636.0 \n",
      "After 2,500 learning games:\n",
      "mean score: 180.0 | max score: 2169.0 \n",
      "After 3,000 learning games:\n",
      "mean score: 198.0 | max score: 3627.0 \n",
      "After 3,500 learning games:\n",
      "mean score: 198.0 | max score: 2736.0 \n",
      "After 4,000 learning games:\n",
      "mean score: 184.5 | max score: 2565.0 \n",
      "After 4,500 learning games:\n",
      "mean score: 189.0 | max score: 5436.0 \n",
      "After 5,000 learning games:\n",
      "mean score: 198.0 | max score: 2871.0 \n",
      "After 5,500 learning games:\n",
      "mean score: 189.0 | max score: 4086.0 \n",
      "After 6,000 learning games:\n",
      "mean score: 198.0 | max score: 2556.0 \n",
      "After 6,500 learning games:\n",
      "mean score: 189.0 | max score: 2511.0 \n",
      "After 7,000 learning games:\n",
      "mean score: 180.0 | max score: 1989.0 \n",
      "After 7,500 learning games:\n",
      "mean score: 189.0 | max score: 2988.0 \n",
      "After 8,000 learning games:\n",
      "mean score: 198.0 | max score: 3366.0 \n",
      "After 8,500 learning games:\n",
      "mean score: 216.0 | max score: 3294.0 \n",
      "After 9,000 learning games:\n",
      "mean score: 198.0 | max score: 3726.0 \n",
      "After 9,500 learning games:\n",
      "mean score: 189.0 | max score: 3267.0 \n",
      "After 10,000 learning games:\n",
      "mean score: 198.0 | max score: 2178.0 \n",
      "After 10,500 learning games:\n",
      "mean score: 193.5 | max score: 2610.0 \n",
      "After 11,000 learning games:\n",
      "mean score: 198.0 | max score: 2817.0 \n",
      "After 11,500 learning games:\n",
      "mean score: 193.5 | max score: 2421.0 \n",
      "After 12,000 learning games:\n",
      "mean score: 184.5 | max score: 6165.0 \n",
      "After 12,500 learning games:\n",
      "mean score: 216.0 | max score: 2898.0 \n",
      "After 13,000 learning games:\n",
      "mean score: 207.0 | max score: 2817.0 \n",
      "After 13,500 learning games:\n",
      "mean score: 198.0 | max score: 2898.0 \n",
      "After 14,000 learning games:\n",
      "mean score: 198.0 | max score: 2403.0 \n",
      "After 14,500 learning games:\n",
      "mean score: 198.0 | max score: 3582.0 \n",
      "After 15,000 learning games:\n",
      "mean score: 180.0 | max score: 5625.0 \n",
      "After 15,500 learning games:\n",
      "mean score: 198.0 | max score: 3663.0 \n",
      "After 16,000 learning games:\n",
      "mean score: 189.0 | max score: 3150.0 \n",
      "After 16,500 learning games:\n",
      "mean score: 198.0 | max score: 3987.0 \n",
      "After 17,000 learning games:\n",
      "mean score: 189.0 | max score: 5382.0 \n",
      "After 17,500 learning games:\n",
      "mean score: 216.0 | max score: 4392.0 \n",
      "After 18,000 learning games:\n",
      "mean score: 229.5 | max score: 3897.0 \n",
      "After 18,500 learning games:\n",
      "mean score: 180.0 | max score: 2538.0 \n",
      "After 19,000 learning games:\n",
      "mean score: 180.0 | max score: 3771.0 \n",
      "After 19,500 learning games:\n",
      "mean score: 198.0 | max score: 2358.0 \n",
      "After 20,000 learning games:\n",
      "mean score: 193.5 | max score: 3132.0 \n",
      "After 20,500 learning games:\n",
      "mean score: 193.5 | max score: 2853.0 \n",
      "After 21,000 learning games:\n",
      "mean score: 207.0 | max score: 5445.0 \n",
      "After 21,500 learning games:\n",
      "mean score: 198.0 | max score: 2772.0 \n",
      "After 22,000 learning games:\n",
      "mean score: 189.0 | max score: 2187.0 \n",
      "After 22,500 learning games:\n",
      "mean score: 189.0 | max score: 4824.0 \n",
      "After 23,000 learning games:\n",
      "mean score: 189.0 | max score: 3150.0 \n",
      "After 23,500 learning games:\n",
      "mean score: 189.0 | max score: 3798.0 \n",
      "After 24,000 learning games:\n",
      "mean score: 207.0 | max score: 1980.0 \n",
      "After 24,500 learning games:\n",
      "mean score: 207.0 | max score: 1980.0 \n",
      "After 25,000 learning games:\n",
      "mean score: 171.0 | max score: 2871.0 \n",
      "After 25,500 learning games:\n",
      "mean score: 198.0 | max score: 4869.0 \n",
      "After 26,000 learning games:\n",
      "mean score: 198.0 | max score: 3753.0 \n",
      "After 26,500 learning games:\n",
      "mean score: 207.0 | max score: 4302.0 \n",
      "After 27,000 learning games:\n",
      "mean score: 198.0 | max score: 2862.0 \n",
      "After 27,500 learning games:\n",
      "mean score: 198.0 | max score: 2601.0 \n",
      "After 28,000 learning games:\n",
      "mean score: 198.0 | max score: 3033.0 \n",
      "After 28,500 learning games:\n",
      "mean score: 189.0 | max score: 2736.0 \n",
      "After 29,000 learning games:\n",
      "mean score: 189.0 | max score: 5508.0 \n",
      "After 29,500 learning games:\n",
      "mean score: 180.0 | max score: 3267.0 \n",
      "After 30,000 learning games:\n",
      "mean score: 180.0 | max score: 2331.0 \n",
      "After 30,500 learning games:\n",
      "mean score: 207.0 | max score: 3447.0 \n",
      "After 31,000 learning games:\n",
      "mean score: 189.0 | max score: 2430.0 \n",
      "After 31,500 learning games:\n",
      "mean score: 198.0 | max score: 5085.0 \n",
      "After 32,000 learning games:\n",
      "mean score: 207.0 | max score: 3312.0 \n",
      "After 32,500 learning games:\n",
      "mean score: 207.0 | max score: 4599.0 \n",
      "After 33,000 learning games:\n",
      "mean score: 180.0 | max score: 3069.0 \n",
      "After 33,500 learning games:\n",
      "mean score: 211.5 | max score: 3015.0 \n",
      "After 34,000 learning games:\n",
      "mean score: 180.0 | max score: 3492.0 \n",
      "After 34,500 learning games:\n",
      "mean score: 189.0 | max score: 3141.0 \n",
      "After 35,000 learning games:\n",
      "mean score: 189.0 | max score: 3825.0 \n",
      "After 35,500 learning games:\n",
      "mean score: 198.0 | max score: 1890.0 \n",
      "After 36,000 learning games:\n",
      "mean score: 207.0 | max score: 2097.0 \n",
      "After 36,500 learning games:\n",
      "mean score: 207.0 | max score: 3411.0 \n",
      "After 37,000 learning games:\n",
      "mean score: 189.0 | max score: 1638.0 \n",
      "After 37,500 learning games:\n",
      "mean score: 198.0 | max score: 5283.0 \n",
      "After 38,000 learning games:\n",
      "mean score: 180.0 | max score: 5571.0 \n",
      "After 38,500 learning games:\n",
      "mean score: 207.0 | max score: 3249.0 \n",
      "After 39,000 learning games:\n",
      "mean score: 189.0 | max score: 3177.0 \n",
      "After 39,500 learning games:\n",
      "mean score: 180.0 | max score: 2934.0 \n",
      "After 40,000 learning games:\n",
      "mean score: 189.0 | max score: 3384.0 \n",
      "After 40,500 learning games:\n",
      "mean score: 189.0 | max score: 3834.0 \n",
      "After 41,000 learning games:\n",
      "mean score: 189.0 | max score: 4239.0 \n",
      "After 41,500 learning games:\n",
      "mean score: 198.0 | max score: 2556.0 \n",
      "After 42,000 learning games:\n",
      "mean score: 198.0 | max score: 2133.0 \n",
      "After 42,500 learning games:\n",
      "mean score: 198.0 | max score: 2124.0 \n",
      "After 43,000 learning games:\n",
      "mean score: 198.0 | max score: 3348.0 \n",
      "After 43,500 learning games:\n",
      "mean score: 216.0 | max score: 2421.0 \n",
      "After 44,000 learning games:\n",
      "mean score: 180.0 | max score: 2583.0 \n",
      "After 44,500 learning games:\n",
      "mean score: 207.0 | max score: 1854.0 \n",
      "After 45,000 learning games:\n",
      "mean score: 207.0 | max score: 3780.0 \n",
      "After 45,500 learning games:\n",
      "mean score: 198.0 | max score: 5193.0 \n",
      "After 46,000 learning games:\n",
      "mean score: 180.0 | max score: 5625.0 \n",
      "After 46,500 learning games:\n",
      "mean score: 189.0 | max score: 2421.0 \n",
      "After 47,000 learning games:\n",
      "mean score: 225.0 | max score: 7434.0 \n",
      "After 47,500 learning games:\n",
      "mean score: 207.0 | max score: 3528.0 \n",
      "After 48,000 learning games:\n",
      "mean score: 193.5 | max score: 2763.0 \n",
      "After 48,500 learning games:\n",
      "mean score: 198.0 | max score: 1899.0 \n",
      "After 49,000 learning games:\n",
      "mean score: 189.0 | max score: 4860.0 \n",
      "After 49,500 learning games:\n",
      "mean score: 189.0 | max score: 2655.0 \n",
      "After 50,000 learning games:\n",
      "mean score: 202.5 | max score: 3195.0 \n",
      "After 50,500 learning games:\n",
      "mean score: 189.0 | max score: 3141.0 \n",
      "After 51,000 learning games:\n",
      "mean score: 198.0 | max score: 5805.0 \n",
      "After 51,500 learning games:\n",
      "mean score: 180.0 | max score: 5283.0 \n",
      "After 52,000 learning games:\n",
      "mean score: 207.0 | max score: 3744.0 \n",
      "After 52,500 learning games:\n",
      "mean score: 193.5 | max score: 3663.0 \n",
      "After 53,000 learning games:\n",
      "mean score: 189.0 | max score: 3114.0 \n",
      "After 53,500 learning games:\n",
      "mean score: 189.0 | max score: 3618.0 \n",
      "After 54,000 learning games:\n",
      "mean score: 198.0 | max score: 3708.0 \n",
      "After 54,500 learning games:\n",
      "mean score: 189.0 | max score: 3105.0 \n",
      "After 55,000 learning games:\n",
      "mean score: 180.0 | max score: 3384.0 \n",
      "After 55,500 learning games:\n",
      "mean score: 198.0 | max score: 2682.0 \n",
      "After 56,000 learning games:\n",
      "mean score: 180.0 | max score: 2097.0 \n",
      "After 56,500 learning games:\n",
      "mean score: 216.0 | max score: 2340.0 \n",
      "After 57,000 learning games:\n",
      "mean score: 189.0 | max score: 3600.0 \n",
      "After 57,500 learning games:\n",
      "mean score: 207.0 | max score: 3906.0 \n",
      "After 58,000 learning games:\n",
      "mean score: 189.0 | max score: 1872.0 \n",
      "After 58,500 learning games:\n",
      "mean score: 198.0 | max score: 2871.0 \n",
      "After 59,000 learning games:\n",
      "mean score: 207.0 | max score: 3699.0 \n",
      "After 59,500 learning games:\n",
      "mean score: 189.0 | max score: 3105.0 \n",
      "After 60,000 learning games:\n",
      "mean score: 216.0 | max score: 4032.0 \n",
      "After 60,500 learning games:\n",
      "mean score: 198.0 | max score: 5760.0 \n",
      "After 61,000 learning games:\n",
      "mean score: 189.0 | max score: 3168.0 \n",
      "After 61,500 learning games:\n",
      "mean score: 198.0 | max score: 4284.0 \n",
      "After 62,000 learning games:\n",
      "mean score: 207.0 | max score: 3366.0 \n",
      "After 62,500 learning games:\n",
      "mean score: 180.0 | max score: 3555.0 \n",
      "After 63,000 learning games:\n",
      "mean score: 198.0 | max score: 4509.0 \n",
      "After 63,500 learning games:\n",
      "mean score: 189.0 | max score: 2871.0 \n",
      "After 64,000 learning games:\n",
      "mean score: 216.0 | max score: 12861.0 \n",
      "After 64,500 learning games:\n",
      "mean score: 189.0 | max score: 2529.0 \n",
      "After 65,000 learning games:\n",
      "mean score: 198.0 | max score: 4824.0 \n",
      "After 65,500 learning games:\n",
      "mean score: 189.0 | max score: 2997.0 \n",
      "After 66,000 learning games:\n",
      "mean score: 216.0 | max score: 3564.0 \n",
      "After 66,500 learning games:\n",
      "mean score: 198.0 | max score: 9450.0 \n",
      "After 67,000 learning games:\n",
      "mean score: 198.0 | max score: 2898.0 \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nQ-learning:\")\n",
    "agent = Agent(Threes)\n",
    "agent, mean_score_learning, max_score_learning = train_qlearning(agent, n_games=500, n_episodes=500,\n",
    "                                                                 n_training_blocks=134, level='Hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FxMCgmLyHMVY",
    "outputId": "59d1ec9b-2bf5-495e-b576-5637188ae899"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean score: 207.0 | max score: 6939.0 \n"
     ]
    }
   ],
   "source": [
    "mean_score_ql, max_score_ql = print_demo_game_stats(\n",
    "    agent, n_games=1000, level='Hard', mode='q-learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V6XOyDuyLf-8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "run_qlearning.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
