{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HCTO0iNG4n9V"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow import keras\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "6KbetwG24tzE",
    "outputId": "a7f81b22-a202-4099-dcdc-cc02d1c834bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of devices:\n",
      "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 881440018777266711),\n",
      " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 4312646848002408110),\n",
      " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 4567475986391415858),\n",
      " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -2259003416087525562),\n",
      " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 6218320445764940763),\n",
      " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 4044634075013356987),\n",
      " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6804726289300748360),\n",
      " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -7541924663223483815),\n",
      " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 4536375203606212004),\n",
      " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 9191021705309272690),\n",
      " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 3281819465280621177)]\n"
     ]
    }
   ],
   "source": [
    "use_tpu = True #@param {type:\"boolean\"}\n",
    "\n",
    "if use_tpu:\n",
    "    assert 'COLAB_TPU_ADDR' in os.environ, 'Missing TPU; did you request a TPU in Notebook Settings?'\n",
    "\n",
    "if 'COLAB_TPU_ADDR' in os.environ:\n",
    "  TF_MASTER = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\n",
    "else:\n",
    "  TF_MASTER=''\n",
    "\n",
    "with tf.compat.v1.Session(TF_MASTER) as session:\n",
    "  print ('List of devices:')\n",
    "  pprint.pprint(session.list_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z2MKEVXX4fab"
   },
   "outputs": [],
   "source": [
    "def movable_condition(first, second):\n",
    "    \"\"\"Define whether two close tile can be merged\"\"\"\n",
    "    return ((first == 0) and (second != 0)) or \\\n",
    "           ((np.any(np.array([first, second]) > 2)) and (first == second)) or \\\n",
    "           ((first, second) == (1, 2)) or \\\n",
    "           ((second, first) == (1, 2))\n",
    "\n",
    "\n",
    "def can_move_col(array):\n",
    "    \"\"\"Check whether an array can be merged.\"\"\"\n",
    "    for i in range(3):\n",
    "        first, second = array[i], array[i + 1]\n",
    "        if movable_condition(first, second):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def allowed_moves(state):\n",
    "    \"\"\"Find allowed moves for a certain game state\"\"\"\n",
    "    allowed_actions = []\n",
    "    # Check whether the agent can swipe up\n",
    "    if np.any([can_move_col(col) for col in state.T]):\n",
    "        allowed_actions.append('w')\n",
    "    # Check whether the agent can swipe down\n",
    "    if np.any([can_move_col(col[::-1]) for col in state.T]):\n",
    "        allowed_actions.append('s')\n",
    "    # Check whether the agent can swipe left\n",
    "    if np.any([can_move_col(row) for row in state]):\n",
    "        allowed_actions.append('a')\n",
    "    # Check whether the agent can swipe right\n",
    "    if np.any([can_move_col(row[::-1]) for row in state]):\n",
    "        allowed_actions.append('d')\n",
    "    return allowed_actions\n",
    "\n",
    "\n",
    "def try_move_col(array):\n",
    "    \"\"\"Return the next state for an array\"\"\"\n",
    "    new_array = array.copy()\n",
    "    for i in range(3):\n",
    "        first, second = array[i], array[i + 1]\n",
    "        if movable_condition(first, second):\n",
    "            new_array[i] = first + second\n",
    "            new_array[i + 1:] = np.append(new_array[i + 2:], 0)\n",
    "            return new_array\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "\n",
    "def get_reward(current_state, next_state):\n",
    "    \"\"\"Given the current state and the next state, return the reward for the transition action.\"\"\"\n",
    "    reward = 0\n",
    "    # maximum number gets larger\n",
    "    reward += (np.max(next_state) - np.max(current_state))\n",
    "    # more merge\n",
    "    reward += (get_score(next_state) - get_score(current_state))\n",
    "    return reward\n",
    "\n",
    "\n",
    "def get_score(state):\n",
    "    \"\"\"Get score given a game state\"\"\"\n",
    "    all_power = [3**(np.log2(num/3)+1) for row in state for num in row if num > 3]\n",
    "    return np.sum(all_power)\n",
    "\n",
    "\n",
    "def try_move(current_state, action):\n",
    "    \"\"\"Given the state and the chosen action, return the next state\"\"\"\n",
    "    next_state = current_state.copy()\n",
    "    allowed_actions = allowed_moves(current_state)\n",
    "    if action not in allowed_actions:\n",
    "        print(f'Can not move {action}')\n",
    "        return current_state\n",
    "\n",
    "    # Swipe up\n",
    "    if action == 'w':\n",
    "        for i, col in enumerate(current_state.T):\n",
    "            if can_move_col(col):\n",
    "                next_state.T[i] = try_move_col(col)\n",
    "    # Swipe down\n",
    "    elif action == 's':\n",
    "        for i, col in enumerate(current_state.T):\n",
    "            if can_move_col(col[::-1]):\n",
    "                new_array = try_move_col(col[::-1])\n",
    "                next_state.T[i] = new_array[::-1]\n",
    "    # Swipe left\n",
    "    elif action == 'a':\n",
    "        for i, col in enumerate(current_state):\n",
    "            if can_move_col(col):\n",
    "                next_state[i] = try_move_col(col)\n",
    "    # Swipe right\n",
    "    elif action == 'd':\n",
    "        for i, col in enumerate(current_state):\n",
    "            if can_move_col(col[::-1]):\n",
    "                new_array = try_move_col(col[::-1])\n",
    "                next_state[i] = new_array[::-1]\n",
    "\n",
    "    elif action == 'stop':\n",
    "        return current_state, get_score(current_state)\n",
    "\n",
    "    reward = get_reward(current_state, next_state)\n",
    "\n",
    "    return next_state, reward\n",
    "\n",
    "\n",
    "def hashable(state):\n",
    "    \"\"\"Switch state matrix to string matrix, so as to make it hashable.\"\"\"\n",
    "    return ', '.join([str(int(i)) for row in state for i in row])\n",
    "\n",
    "\n",
    "def _select_best_move(game):\n",
    "    \"\"\"Selects best move that can get the maximum reward for the next state\"\"\"\n",
    "    possible_next_actions = allowed_moves(game.state)\n",
    "    state_action_score = [(move, try_move(game.state, move)[1])\n",
    "                          for move in possible_next_actions]\n",
    "    max_score = max(state_action_score, key=lambda item: item[1])[1]\n",
    "    max_move_list = [move for move, score in state_action_score if score == max_score]\n",
    "    best_next_move = np.random.choice(max_move_list)\n",
    "    return best_next_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AVl-5GHm4fad"
   },
   "outputs": [],
   "source": [
    "class Threes:\n",
    "    \"\"\"\n",
    "    This is a simulated environment of game Threes.\n",
    "    Swipe direction: {left: 'a', right: 'd', up: 'w', down: 's'}.\n",
    "    There are two levels for this game: ['hard', 'easy'], in which default level is 'hard'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, level='hard'):\n",
    "        \"\"\"Initialize the game\"\"\"\n",
    "        self.state = np.zeros((4, 4))\n",
    "        x, y = np.random.choice(4, 2)\n",
    "        self.state[x, y] = np.random.choice([1, 2])\n",
    "        self.score = get_score(self.state)\n",
    "        self.level = level\n",
    "\n",
    "    def playable(self):\n",
    "        \"\"\"Check whether the game is still playable.\"\"\"\n",
    "        if len(allowed_moves(self.state)) != 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def gen_new_tile(self):\n",
    "        \"\"\"Generate a new tile after each move.\"\"\"\n",
    "\n",
    "        # Basic list of numbers that can be selected\n",
    "        choice_list = [1, 2, 3]\n",
    "\n",
    "        # More number can be selected when the maximum number on the grid gets larger\n",
    "        if np.max(self.state) % 3 == 0:\n",
    "            max_power = np.int(np.log2(np.max(self.state) / 3))\n",
    "            choice_list += [3 * 2 ** i for i in range(max_power + 1)]\n",
    "\n",
    "        # Generate the probabilities for each candidate\n",
    "        if self.level == 'hard':\n",
    "            norm_prob = [1 / len(choice_list)] * len(choice_list)\n",
    "        else:\n",
    "            prob = [i + 1 for i in range(len(choice_list))][::-1]\n",
    "            norm_prob = [num / sum(prob) for num in prob]\n",
    "\n",
    "            # return next number\n",
    "        return np.random.choice(choice_list, p=norm_prob)\n",
    "\n",
    "    def make_move(self, action):\n",
    "        \"\"\"Given the action, the game goes to the next state\"\"\"\n",
    "        if action == 'stop':\n",
    "            self.score = get_score(self.state)\n",
    "            return self.state, self.score\n",
    "\n",
    "        self.state = try_move(self.state, action)[0]\n",
    "\n",
    "        # generate new tile for the current state\n",
    "        new_tile = self.gen_new_tile()\n",
    "        loc_0 = np.argwhere(self.state == 0)\n",
    "        x, y = loc_0[np.random.choice(len(loc_0))]\n",
    "\n",
    "        # Update the game state and scores\n",
    "        self.state[x, y] = new_tile\n",
    "        self.score = get_score(self.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dtigyhzn4faf"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"\n",
    "    This is an agent to play game \"Threes\". There are two main mode to play the game. One is human mode and the other\n",
    "    is computer mode(demo game). For the computer mode, there are currently three methods to play the game: [\n",
    "    'random', 'max', 'q-learning'] The functions here are inspired by\n",
    "    \"https://github.com/brianspiering/rl-course/blob/master/labs/lab_4_tic_tac_toe/lab_4_tic_tac_toe.ipynb\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, threes, epsilon=0.1, alpha=1.0):\n",
    "        \"\"\"Initial the Agent.\"\"\"\n",
    "        self.V = dict()\n",
    "        self.NewGame = threes\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def state_value(self, game_state, action):\n",
    "        \"\"\"Look up state value. If never seen state, then assume 0.\"\"\"\n",
    "        return self.V.get((hashable(game_state), action), 0.0)\n",
    "\n",
    "    def state_values(self, game_state, actions):\n",
    "        \"\"\"Return a dictionary of state-value pair. It is for finding the action that can maximize the q value \"\"\"\n",
    "        return dict(((hashable(game_state), action), self.state_value(game_state, action)) for action in actions)\n",
    "\n",
    "    def learn_game(self, n_episodes=1000):\n",
    "        \"\"\"Let's learn through complete experience to get that reward.\"\"\"\n",
    "        for e in range(1, n_episodes + 1):\n",
    "            game = self.NewGame()\n",
    "            while game.playable():\n",
    "                action, reward = self.learn_from_move(game)\n",
    "            self.V[(hashable(game.state), action)] = reward\n",
    "\n",
    "    def learn_from_move(self, game):\n",
    "        \"\"\"The heart of Q-learning.\"\"\"\n",
    "\n",
    "        current_state = game.state\n",
    "        # Select next action with epsilon-greedy method\n",
    "        selected_move = self.learn_select_move(game)\n",
    "\n",
    "        # Next state s(t+1) and reward r\n",
    "        next_state, reward = try_move(current_state, selected_move)\n",
    "\n",
    "        # Current state Q value Q(s, a)\n",
    "        old_value = self.state_value(current_state, selected_move)\n",
    "\n",
    "        # best action a* for the next state with the largest q value Q(st+1, a*)\n",
    "        next_max_V, next_max_move = self.select_best_move(game, next_state)\n",
    "\n",
    "        # Q-learning that updates the q-value\n",
    "        self.V[(hashable(current_state), selected_move)] = (1 - self.alpha) * old_value + self.alpha * (\n",
    "                    reward + next_max_V)\n",
    "\n",
    "        game.make_move(selected_move)\n",
    "        return selected_move, reward\n",
    "\n",
    "    def learn_select_move(self, game):\n",
    "        \"\"\"Exploration and exploitation\"\"\"\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            selected_action = np.random.choice(allowed_moves(game.state))\n",
    "        else:\n",
    "            selected_action = self.select_best_move(game, game.state)[1]\n",
    "        return selected_action\n",
    "\n",
    "    def select_best_move(self, game, game_state):\n",
    "        \"\"\"Selects best move for given state(Greedy)\"\"\"\n",
    "        state_action_values = self.state_values(game_state, allowed_moves(game_state))\n",
    "        max_V = max(state_action_values.values())\n",
    "        max_move = np.random.choice([state_action[1] for state_action, v in state_action_values.items() if v == max_V])\n",
    "        return max_V, max_move\n",
    "\n",
    "    def demo_game(self, level='hard', mode='random'):\n",
    "        \"\"\"Agent plays with different policies (random/max/q-learning)\"\"\"\n",
    "        game = self.NewGame()\n",
    "        game.level = level\n",
    "        while game.playable():\n",
    "            if mode == 'random':\n",
    "                next_action = np.random.choice(allowed_moves(game.state))\n",
    "            elif mode == 'max':\n",
    "                next_action = _select_best_move(game)\n",
    "            elif mode == 'q-learning':\n",
    "                next_action = self.select_best_move(game, game.state)[1]\n",
    "            else:\n",
    "                return \"No such mode\"\n",
    "            game.make_move(next_action)\n",
    "        return game.score\n",
    "\n",
    "    def human_mode(self):\n",
    "        \"\"\"Interactive mode\"\"\"\n",
    "        game = self.NewGame()\n",
    "        level = input('level: easy or hard?')\n",
    "        game.level = level\n",
    "        print(game.state)\n",
    "        while game.playable():\n",
    "            human_allowed_moves = allowed_moves(game.state) + ['stop']\n",
    "            human_move = input(f'You can input {human_allowed_moves}')\n",
    "            if human_move == 'stop':\n",
    "                return f'Game over! Your score is {game.score}'\n",
    "            game.make_move(human_move)\n",
    "            print(game.state)\n",
    "        return f'Game over! Your score is {game.score}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vfI4m82m5P9p"
   },
   "outputs": [],
   "source": [
    "def print_demo_game_stats(agent, n_games=100, level='Hard', mode='random'):\n",
    "    \"\"\"print the result(mean score and max score) of playing demo game for some times\"\"\"\n",
    "    results = [agent.demo_game(level=level, mode=mode) for _ in range(n_games)]\n",
    "    mean_score, max_score = np.mean(results), np.max(results)\n",
    "    print(f\"mean score: {mean_score}\")\n",
    "    print(f\"max score: {max_score}\")\n",
    "    return mean_score, max_score\n",
    "\n",
    "\n",
    "def train_qlearning(agent, n_games=100, n_episodes=100, n_training_blocks=10, level='Hard'):\n",
    "    \"\"\"Given agent, do more training. Return (hopefully) improved agent.\"\"\"\n",
    "    for n_training_block in range(1, n_training_blocks + 1):\n",
    "        agent.learn_game(n_episodes)\n",
    "        print(f\"After {n_episodes * n_training_block:,} learning games:\")\n",
    "        mean_score, max_score = print_demo_game_stats(agent, n_games=n_games, level=level, mode='q-learning')\n",
    "    return agent, mean_score, max_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T06:57:48.344579Z",
     "start_time": "2020-06-22T06:57:48.337411Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "qGshLQLe4fah"
   },
   "outputs": [],
   "source": [
    "# Without learning, machine plays by choosing the move that maximize the score of next state.\n",
    "agent = Agent(Threes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T06:58:03.843531Z",
     "start_time": "2020-06-22T06:57:48.350590Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "INDhpwja4faj",
    "outputId": "dde54f49-d757-473a-aa04-40def98fdbe2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No training:\n",
      " Random Mode:\n",
      "mean score: 327.6\n",
      "max score: 2502.0\n",
      "Greedy Mode:\n",
      "mean score: 446.76\n",
      "max score: 4122.0\n"
     ]
    }
   ],
   "source": [
    "print(\"No training:\\n Random Mode:\")\n",
    "mean_score_random, max_score_random = print_demo_game_stats(agent, n_games=100, level='Hard', mode='random')\n",
    "print('Greedy Mode:')\n",
    "mean_score_max, max_score_max = print_demo_game_stats(agent, n_games=100, level='Hard', mode='max')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T06:36:16.739279Z",
     "start_time": "2020-06-22T06:36:16.735761Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "0uET0hzk4fao"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T06:36:17.195816Z",
     "start_time": "2020-06-22T06:36:17.184059Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "MDjcGsEv4fas"
   },
   "outputs": [],
   "source": [
    "class ExperienceReplay():\n",
    "    \"Store the agent's experiences inorder to collect enough example to get a reward signal.\"\n",
    "    \n",
    "    def __init__(self, max_memory=100):\n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "\n",
    "    def remember(self, states, game_over):\n",
    "        self.memory.append([states, game_over])\n",
    "        \n",
    "        # If memory is too large, then evict to reduce memory size\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            # Evict oldest\n",
    "            del self.memory[0]\n",
    "            \n",
    "    def get_batch(self, model, batch_size=10):\n",
    "        len_memory = len(self.memory)\n",
    "        # Dimension of the output layer of NN\n",
    "        n_actions = model.layers[1].output_shape[1]\n",
    "        # Dimension of the input layer of NN\n",
    "        env_dim = model.layers[0].input_shape[1]\n",
    "        \n",
    "        inputs = np.zeros((min(len_memory, batch_size), env_dim))\n",
    "        targets = np.zeros((inputs.shape[0], n_actions))\n",
    "        \n",
    "        for i, idx in enumerate(np.random.randint(0, len_memory, size=inputs.shape[0])):\n",
    "            state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]\n",
    "            game_continue = self.memory[idx][1]\n",
    "            # train_x: st \n",
    "            inputs[i:i+1] = state_t\n",
    "            # Initialize target y: q values Q(st, a0), Q(st, a1), Q(st, a2) for each action\n",
    "            targets[i] = model.predict(state_t)[0]\n",
    "            # Next max q value for the next state: maxQ(stp1, a*)\n",
    "            q_sa = np.max(model.predict(state_tp1))\n",
    "            if game_continue:\n",
    "                # Update Q(st,at) with Q-learning \n",
    "                targets[i, action_t] = reward_t + q_sa\n",
    "            else:\n",
    "                targets[i, action_t] = reward_t\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T07:07:23.922681Z",
     "start_time": "2020-06-22T07:07:23.888413Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "7w5xQXgd4fau",
    "outputId": "5fbfefb5-9737-4749-fe05-be9b1de43c4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 676\n",
      "Trainable params: 676\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, InputLayer\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(InputLayer(input_shape=(16,)))\n",
    "\n",
    "# The second layer (hidden)\n",
    "model.add(Dense(units=32, activation='relu')) \n",
    "\n",
    "# # Second hidden layer\n",
    "# model.add(Dense(units=64,activation='relu')) \n",
    "\n",
    "# # Third hidden layer\n",
    "# model.add(Dense(units=64,activation='relu')) \n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(units=4,activation='softmax')) \n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss=\"categorical_crossentropy\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T06:36:24.215253Z",
     "start_time": "2020-06-22T06:36:24.196640Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "jq1Pszwk4faw"
   },
   "outputs": [],
   "source": [
    "def hash_num(state):\n",
    "    return np.array([[int(i) for row in state for i in row]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T06:55:18.313228Z",
     "start_time": "2020-06-22T06:46:26.998263Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "QkZL7z1e4fay",
    "outputId": "66e49244-fe79-4bd0-99e5-d702eec7e619"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/1,001 | Loss value: 32.135 | Mean score: 207.0 | Max score: 207.0\n",
      "Epoch: 002/1,001 | Loss value: 11.065 | Mean score: 162.0 | Max score: 207.0\n",
      "Epoch: 003/1,001 | Loss value: 10.442 | Mean score: 174.0 | Max score: 207.0\n",
      "Epoch: 004/1,001 | Loss value:  7.267 | Mean score: 218.25 | Max score: 351.0\n",
      "Epoch: 005/1,001 | Loss value: 44.590 | Mean score: 210.6 | Max score: 351.0\n",
      "Epoch: 006/1,001 | Loss value: 56.962 | Mean score: 310.5 | Max score: 810.0\n",
      "Epoch: 007/1,001 | Loss value: 45.314 | Mean score: 389.57142857142856 | Max score: 864.0\n",
      "Epoch: 008/1,001 | Loss value: 110.463 | Mean score: 380.25 | Max score: 864.0\n",
      "Epoch: 009/1,001 | Loss value: 356.574 | Mean score: 349.0 | Max score: 864.0\n",
      "Epoch: 010/1,001 | Loss value: 93.510 | Mean score: 383.4 | Max score: 864.0\n",
      "Epoch: 011/1,001 | Loss value: 722.636 | Mean score: 373.90909090909093 | Max score: 864.0\n",
      "Epoch: 012/1,001 | Loss value: 82.391 | Mean score: 355.5 | Max score: 864.0\n",
      "Epoch: 013/1,001 | Loss value: 1601.579 | Mean score: 346.15384615384613 | Max score: 864.0\n",
      "Epoch: 014/1,001 | Loss value: 387.212 | Mean score: 364.5 | Max score: 864.0\n",
      "Epoch: 015/1,001 | Loss value: 824.769 | Mean score: 342.6 | Max score: 864.0\n",
      "Epoch: 016/1,001 | Loss value: 286.080 | Mean score: 347.0625 | Max score: 864.0\n",
      "Epoch: 017/1,001 | Loss value: 1794.886 | Mean score: 369.0 | Max score: 864.0\n",
      "Epoch: 018/1,001 | Loss value: 968.397 | Mean score: 355.0 | Max score: 864.0\n",
      "Epoch: 019/1,001 | Loss value: 1512.445 | Mean score: 348.1578947368421 | Max score: 864.0\n",
      "Epoch: 020/1,001 | Loss value: 2186.797 | Mean score: 344.25 | Max score: 864.0\n",
      "Epoch: 021/1,001 | Loss value: 526.362 | Mean score: 375.0 | Max score: 990.0\n",
      "Epoch: 022/1,001 | Loss value: 2159.553 | Mean score: 365.3181818181818 | Max score: 990.0\n",
      "Epoch: 023/1,001 | Loss value: 3637.476 | Mean score: 370.95652173913044 | Max score: 990.0\n",
      "Epoch: 024/1,001 | Loss value: 1063.000 | Mean score: 718.125 | Max score: 8703.0\n",
      "Epoch: 025/1,001 | Loss value: 2366.847 | Mean score: 709.56 | Max score: 8703.0\n",
      "Epoch: 026/1,001 | Loss value: 1589.421 | Mean score: 698.5384615384615 | Max score: 8703.0\n",
      "Epoch: 027/1,001 | Loss value: 3552.787 | Mean score: 772.0 | Max score: 8703.0\n",
      "Epoch: 028/1,001 | Loss value: 18376.125 | Mean score: 924.75 | Max score: 8703.0\n",
      "Epoch: 029/1,001 | Loss value: 2790.685 | Mean score: 897.8275862068965 | Max score: 8703.0\n",
      "Epoch: 030/1,001 | Loss value: 4782.545 | Mean score: 887.1 | Max score: 8703.0\n",
      "Epoch: 031/1,001 | Loss value: 791.628 | Mean score: 872.7096774193549 | Max score: 8703.0\n",
      "Epoch: 032/1,001 | Loss value: 2164.413 | Mean score: 875.8125 | Max score: 8703.0\n",
      "Epoch: 033/1,001 | Loss value: 810.172 | Mean score: 855.2727272727273 | Max score: 8703.0\n",
      "Epoch: 034/1,001 | Loss value: 2769.386 | Mean score: 834.6176470588235 | Max score: 8703.0\n",
      "Epoch: 035/1,001 | Loss value: 19176.086 | Mean score: 826.4571428571429 | Max score: 8703.0\n",
      "Epoch: 036/1,001 | Loss value: 828.374 | Mean score: 851.25 | Max score: 8703.0\n",
      "Epoch: 037/1,001 | Loss value: 15406.719 | Mean score: 847.9459459459459 | Max score: 8703.0\n",
      "Epoch: 038/1,001 | Loss value: 13349.912 | Mean score: 842.6842105263158 | Max score: 8703.0\n",
      "Epoch: 039/1,001 | Loss value: 43772.895 | Mean score: 875.7692307692307 | Max score: 8703.0\n",
      "Epoch: 040/1,001 | Loss value: 1976.860 | Mean score: 861.75 | Max score: 8703.0\n",
      "Epoch: 041/1,001 | Loss value: 3688.926 | Mean score: 880.2439024390244 | Max score: 8703.0\n",
      "Epoch: 042/1,001 | Loss value: 19302.346 | Mean score: 875.1428571428571 | Max score: 8703.0\n",
      "Epoch: 043/1,001 | Loss value: 30325.484 | Mean score: 867.3488372093024 | Max score: 8703.0\n",
      "Epoch: 044/1,001 | Loss value: 4023.606 | Mean score: 853.3636363636364 | Max score: 8703.0\n",
      "Epoch: 045/1,001 | Loss value: 2723.308 | Mean score: 843.6 | Max score: 8703.0\n",
      "Epoch: 046/1,001 | Loss value: 10172.030 | Mean score: 853.2391304347826 | Max score: 8703.0\n",
      "Epoch: 047/1,001 | Loss value: 15122.780 | Mean score: 838.1489361702128 | Max score: 8703.0\n",
      "Epoch: 048/1,001 | Loss value: 48870.766 | Mean score: 824.25 | Max score: 8703.0\n",
      "Epoch: 049/1,001 | Loss value: 6912.900 | Mean score: 818.265306122449 | Max score: 8703.0\n",
      "Epoch: 050/1,001 | Loss value: 12747.562 | Mean score: 808.02 | Max score: 8703.0\n",
      "Epoch: 051/1,001 | Loss value: 31340.686 | Mean score: 796.7647058823529 | Max score: 8703.0\n",
      "Epoch: 052/1,001 | Loss value: 5002.104 | Mean score: 789.4038461538462 | Max score: 8703.0\n",
      "Epoch: 053/1,001 | Loss value: 4044.285 | Mean score: 779.9433962264151 | Max score: 8703.0\n",
      "Epoch: 054/1,001 | Loss value: 9119.031 | Mean score: 782.0 | Max score: 8703.0\n",
      "Epoch: 055/1,001 | Loss value: 2150.965 | Mean score: 768.6 | Max score: 8703.0\n",
      "Epoch: 056/1,001 | Loss value: 1978.068 | Mean score: 760.6607142857143 | Max score: 8703.0\n",
      "Epoch: 057/1,001 | Loss value: 1038.770 | Mean score: 755.2105263157895 | Max score: 8703.0\n",
      "Epoch: 058/1,001 | Loss value: 3114.589 | Mean score: 743.7413793103449 | Max score: 8703.0\n",
      "Epoch: 059/1,001 | Loss value: 6503.811 | Mean score: 732.6610169491526 | Max score: 8703.0\n",
      "Epoch: 060/1,001 | Loss value: 1965.972 | Mean score: 722.1 | Max score: 8703.0\n",
      "Epoch: 061/1,001 | Loss value: 4266.877 | Mean score: 719.5573770491803 | Max score: 8703.0\n",
      "Epoch: 062/1,001 | Loss value: 15504.553 | Mean score: 710.8548387096774 | Max score: 8703.0\n",
      "Epoch: 063/1,001 | Loss value: 20349.633 | Mean score: 840.2857142857143 | Max score: 8865.0\n",
      "Epoch: 064/1,001 | Loss value: 64751.762 | Mean score: 829.828125 | Max score: 8865.0\n",
      "Epoch: 065/1,001 | Loss value: 6858.697 | Mean score: 832.2923076923076 | Max score: 8865.0\n",
      "Epoch: 066/1,001 | Loss value: 2824.853 | Mean score: 831.6818181818181 | Max score: 8865.0\n",
      "Epoch: 067/1,001 | Loss value: 8896.611 | Mean score: 837.9402985074627 | Max score: 8865.0\n",
      "Epoch: 068/1,001 | Loss value: 3782.547 | Mean score: 833.2941176470588 | Max score: 8865.0\n",
      "Epoch: 069/1,001 | Loss value: 8534.742 | Mean score: 831.9130434782609 | Max score: 8865.0\n",
      "Epoch: 070/1,001 | Loss value: 9462.451 | Mean score: 826.0714285714286 | Max score: 8865.0\n",
      "Epoch: 071/1,001 | Loss value: 8290.809 | Mean score: 819.5070422535211 | Max score: 8865.0\n",
      "Epoch: 072/1,001 | Loss value: 12856.885 | Mean score: 825.625 | Max score: 8865.0\n",
      "Epoch: 073/1,001 | Loss value: 11736.809 | Mean score: 818.2602739726027 | Max score: 8865.0\n",
      "Epoch: 074/1,001 | Loss value: 10560.175 | Mean score: 815.8378378378378 | Max score: 8865.0\n",
      "Epoch: 075/1,001 | Loss value: 24549.242 | Mean score: 818.52 | Max score: 8865.0\n",
      "Epoch: 076/1,001 | Loss value: 24537.789 | Mean score: 817.3421052631579 | Max score: 8865.0\n",
      "Epoch: 077/1,001 | Loss value: 1586.821 | Mean score: 808.5974025974026 | Max score: 8865.0\n",
      "Epoch: 078/1,001 | Loss value: 2866.055 | Mean score: 808.0384615384615 | Max score: 8865.0\n",
      "Epoch: 079/1,001 | Loss value: 4223.976 | Mean score: 802.367088607595 | Max score: 8865.0\n",
      "Epoch: 080/1,001 | Loss value: 11193.702 | Mean score: 795.9375 | Max score: 8865.0\n",
      "Epoch: 081/1,001 | Loss value: 17633.547 | Mean score: 792.1111111111111 | Max score: 8865.0\n",
      "Epoch: 082/1,001 | Loss value: 3863.907 | Mean score: 792.8780487804878 | Max score: 8865.0\n",
      "Epoch: 083/1,001 | Loss value: 17330.578 | Mean score: 788.4216867469879 | Max score: 8865.0\n",
      "Epoch: 084/1,001 | Loss value: 466573.812 | Mean score: 780.8571428571429 | Max score: 8865.0\n",
      "Epoch: 085/1,001 | Loss value: 8810.081 | Mean score: 772.5176470588235 | Max score: 8865.0\n",
      "Epoch: 086/1,001 | Loss value: 28821.715 | Mean score: 768.9767441860465 | Max score: 8865.0\n",
      "Epoch: 087/1,001 | Loss value: 1558.329 | Mean score: 765.1034482758621 | Max score: 8865.0\n",
      "Epoch: 088/1,001 | Loss value: 25044.475 | Mean score: 764.8977272727273 | Max score: 8865.0\n",
      "Epoch: 089/1,001 | Loss value: 12429.380 | Mean score: 765.9101123595506 | Max score: 8865.0\n",
      "Epoch: 090/1,001 | Loss value: 3614.638 | Mean score: 760.3 | Max score: 8865.0\n",
      "Epoch: 091/1,001 | Loss value: 30653.082 | Mean score: 755.010989010989 | Max score: 8865.0\n",
      "Epoch: 092/1,001 | Loss value: 15460.916 | Mean score: 764.804347826087 | Max score: 8865.0\n",
      "Epoch: 093/1,001 | Loss value: 28192.035 | Mean score: 763.0645161290323 | Max score: 8865.0\n",
      "Epoch: 094/1,001 | Loss value: 3389.362 | Mean score: 757.3404255319149 | Max score: 8865.0\n",
      "Epoch: 095/1,001 | Loss value: 32526.363 | Mean score: 752.3052631578947 | Max score: 8865.0\n",
      "Epoch: 096/1,001 | Loss value: 546.506 | Mean score: 755.34375 | Max score: 8865.0\n",
      "Epoch: 097/1,001 | Loss value: 33583.797 | Mean score: 758.319587628866 | Max score: 8865.0\n",
      "Epoch: 098/1,001 | Loss value: 1862.325 | Mean score: 755.265306122449 | Max score: 8865.0\n",
      "Epoch: 099/1,001 | Loss value: 11999.987 | Mean score: 750.9090909090909 | Max score: 8865.0\n",
      "Epoch: 100/1,001 | Loss value: 16763.824 | Mean score: 762.66 | Max score: 8865.0\n",
      "Epoch: 101/1,001 | Loss value: 9640.610 | Mean score: 757.8712871287129 | Max score: 8865.0\n",
      "Epoch: 102/1,001 | Loss value: 13443.553 | Mean score: 757.9411764705883 | Max score: 8865.0\n",
      "Epoch: 103/1,001 | Loss value: 21041.365 | Mean score: 755.0388349514564 | Max score: 8865.0\n",
      "Epoch: 104/1,001 | Loss value: 13330.803 | Mean score: 750.5480769230769 | Max score: 8865.0\n",
      "Epoch: 105/1,001 | Loss value: 2649.188 | Mean score: 755.5714285714286 | Max score: 8865.0\n",
      "Epoch: 106/1,001 | Loss value: 20971.859 | Mean score: 751.2452830188679 | Max score: 8865.0\n",
      "Epoch: 107/1,001 | Loss value: 2451.506 | Mean score: 762.2242990654206 | Max score: 8865.0\n",
      "Epoch: 108/1,001 | Loss value: 7016.331 | Mean score: 758.25 | Max score: 8865.0\n",
      "Epoch: 109/1,001 | Loss value: 2045.012 | Mean score: 756.1651376146789 | Max score: 8865.0\n",
      "Epoch: 110/1,001 | Loss value: 3617.929 | Mean score: 751.1727272727272 | Max score: 8865.0\n",
      "Epoch: 111/1,001 | Loss value: 104423.062 | Mean score: 747.8108108108108 | Max score: 8865.0\n",
      "Epoch: 112/1,001 | Loss value: 3924.573 | Mean score: 747.4821428571429 | Max score: 8865.0\n",
      "Epoch: 113/1,001 | Loss value: 8719.947 | Mean score: 748.0353982300885 | Max score: 8865.0\n",
      "Epoch: 114/1,001 | Loss value: 8158.638 | Mean score: 783.1578947368421 | Max score: 8865.0\n",
      "Epoch: 115/1,001 | Loss value: 43205.559 | Mean score: 778.4608695652174 | Max score: 8865.0\n",
      "Epoch: 116/1,001 | Loss value: 6810.796 | Mean score: 773.2241379310345 | Max score: 8865.0\n",
      "Epoch: 117/1,001 | Loss value: 4699.966 | Mean score: 804.2307692307693 | Max score: 8865.0\n",
      "Epoch: 118/1,001 | Loss value: 6594.466 | Mean score: 813.2796610169491 | Max score: 8865.0\n",
      "Epoch: 119/1,001 | Loss value: 10141.466 | Mean score: 808.563025210084 | Max score: 8865.0\n",
      "Epoch: 120/1,001 | Loss value: 919409.188 | Mean score: 804.375 | Max score: 8865.0\n",
      "Epoch: 121/1,001 | Loss value: 18130.307 | Mean score: 804.0495867768595 | Max score: 8865.0\n",
      "Epoch: 122/1,001 | Loss value: 10373.861 | Mean score: 814.1311475409836 | Max score: 8865.0\n",
      "Epoch: 123/1,001 | Loss value: 10609.756 | Mean score: 809.2682926829268 | Max score: 8865.0\n",
      "Epoch: 124/1,001 | Loss value: 6620.310 | Mean score: 804.266129032258 | Max score: 8865.0\n",
      "Epoch: 125/1,001 | Loss value: 40687.129 | Mean score: 800.28 | Max score: 8865.0\n",
      "Epoch: 126/1,001 | Loss value: 1353.761 | Mean score: 796.7142857142857 | Max score: 8865.0\n",
      "Epoch: 127/1,001 | Loss value: 2413.859 | Mean score: 795.2598425196851 | Max score: 8865.0\n",
      "Epoch: 128/1,001 | Loss value: 10238.755 | Mean score: 798.5390625 | Max score: 8865.0\n",
      "Epoch: 129/1,001 | Loss value: 6708.119 | Mean score: 794.3720930232558 | Max score: 8865.0\n",
      "Epoch: 130/1,001 | Loss value: 12109.303 | Mean score: 806.4 | Max score: 8865.0\n",
      "Epoch: 131/1,001 | Loss value: 6975.507 | Mean score: 805.1908396946565 | Max score: 8865.0\n",
      "Epoch: 132/1,001 | Loss value: 11312.318 | Mean score: 802.1590909090909 | Max score: 8865.0\n",
      "Epoch: 133/1,001 | Loss value: 17512.639 | Mean score: 799.375939849624 | Max score: 8865.0\n",
      "Epoch: 134/1,001 | Loss value: 43003.324 | Mean score: 795.0895522388059 | Max score: 8865.0\n",
      "Epoch: 135/1,001 | Loss value: 1009.363 | Mean score: 793.3333333333334 | Max score: 8865.0\n",
      "Epoch: 136/1,001 | Loss value: 39014.656 | Mean score: 790.6764705882352 | Max score: 8865.0\n",
      "Epoch: 137/1,001 | Loss value: 49635.816 | Mean score: 789.6350364963504 | Max score: 8865.0\n",
      "Epoch: 138/1,001 | Loss value: 1846.277 | Mean score: 784.6304347826087 | Max score: 8865.0\n",
      "Epoch: 139/1,001 | Loss value: 11119.395 | Mean score: 780.3453237410072 | Max score: 8865.0\n",
      "Epoch: 140/1,001 | Loss value: 12072.768 | Mean score: 776.6357142857142 | Max score: 8865.0\n",
      "Epoch: 141/1,001 | Loss value: 19650.168 | Mean score: 774.2553191489362 | Max score: 8865.0\n",
      "Epoch: 142/1,001 | Loss value: 55896.414 | Mean score: 779.4507042253521 | Max score: 8865.0\n",
      "Epoch: 143/1,001 | Loss value: 16898.262 | Mean score: 775.951048951049 | Max score: 8865.0\n",
      "Epoch: 144/1,001 | Loss value: 6513.799 | Mean score: 772.125 | Max score: 8865.0\n",
      "Epoch: 145/1,001 | Loss value: 35526.898 | Mean score: 770.648275862069 | Max score: 8865.0\n",
      "Epoch: 146/1,001 | Loss value: 11454.045 | Mean score: 769.6849315068494 | Max score: 8865.0\n",
      "Epoch: 147/1,001 | Loss value: 4395.696 | Mean score: 770.0816326530612 | Max score: 8865.0\n",
      "Epoch: 148/1,001 | Loss value: 95623.656 | Mean score: 776.3108108108108 | Max score: 8865.0\n",
      "Epoch: 149/1,001 | Loss value: 12234.161 | Mean score: 772.0671140939597 | Max score: 8865.0\n",
      "Epoch: 150/1,001 | Loss value: 52205.453 | Mean score: 824.82 | Max score: 8865.0\n",
      "Epoch: 151/1,001 | Loss value: 2876.484 | Mean score: 822.3973509933775 | Max score: 8865.0\n",
      "Epoch: 152/1,001 | Loss value: 59747.109 | Mean score: 820.0065789473684 | Max score: 8865.0\n",
      "Epoch: 153/1,001 | Loss value: 43377.145 | Mean score: 816.4705882352941 | Max score: 8865.0\n",
      "Epoch: 154/1,001 | Loss value: 26868.793 | Mean score: 816.1363636363636 | Max score: 8865.0\n",
      "Epoch: 155/1,001 | Loss value: 17735.885 | Mean score: 811.3935483870968 | Max score: 8865.0\n",
      "Epoch: 156/1,001 | Loss value: 16836.920 | Mean score: 807.2884615384615 | Max score: 8865.0\n",
      "Epoch: 157/1,001 | Loss value: 3253.916 | Mean score: 803.1210191082803 | Max score: 8865.0\n",
      "Epoch: 158/1,001 | Loss value: 54708.504 | Mean score: 799.9746835443038 | Max score: 8865.0\n",
      "Epoch: 159/1,001 | Loss value: 71378.289 | Mean score: 804.4528301886793 | Max score: 8865.0\n",
      "Epoch: 160/1,001 | Loss value: 38247.344 | Mean score: 799.7625 | Max score: 8865.0\n",
      "Epoch: 161/1,001 | Loss value: 69929.422 | Mean score: 796.304347826087 | Max score: 8865.0\n",
      "Epoch: 162/1,001 | Loss value: 4643.223 | Mean score: 793.2222222222222 | Max score: 8865.0\n",
      "Epoch: 163/1,001 | Loss value: 17810.012 | Mean score: 790.6748466257669 | Max score: 8865.0\n",
      "Epoch: 164/1,001 | Loss value: 2698.220 | Mean score: 786.7865853658536 | Max score: 8865.0\n",
      "Epoch: 165/1,001 | Loss value: 4500.801 | Mean score: 782.4 | Max score: 8865.0\n",
      "Epoch: 166/1,001 | Loss value: 43447.695 | Mean score: 779.6927710843373 | Max score: 8865.0\n",
      "Epoch: 167/1,001 | Loss value: 21503.770 | Mean score: 775.8862275449102 | Max score: 8865.0\n",
      "Epoch: 168/1,001 | Loss value: 56242.602 | Mean score: 772.7142857142857 | Max score: 8865.0\n",
      "Epoch: 169/1,001 | Loss value: 22359.391 | Mean score: 772.4023668639053 | Max score: 8865.0\n",
      "Epoch: 170/1,001 | Loss value: 1154.874 | Mean score: 769.7117647058824 | Max score: 8865.0\n",
      "Epoch: 171/1,001 | Loss value: 78803.328 | Mean score: 774.1578947368421 | Max score: 8865.0\n",
      "Epoch: 172/1,001 | Loss value: 18235.824 | Mean score: 775.2558139534884 | Max score: 8865.0\n",
      "Epoch: 173/1,001 | Loss value: 17361.342 | Mean score: 771.7630057803468 | Max score: 8865.0\n",
      "Epoch: 174/1,001 | Loss value: 63039.926 | Mean score: 771.3620689655172 | Max score: 8865.0\n",
      "Epoch: 175/1,001 | Loss value: 11893.956 | Mean score: 767.9828571428571 | Max score: 8865.0\n",
      "Epoch: 176/1,001 | Loss value: 26930.207 | Mean score: 765.5625 | Max score: 8865.0\n",
      "Epoch: 177/1,001 | Loss value: 17595.787 | Mean score: 782.0847457627119 | Max score: 8865.0\n",
      "Epoch: 178/1,001 | Loss value: 20865.213 | Mean score: 782.7471910112359 | Max score: 8865.0\n",
      "Epoch: 179/1,001 | Loss value: 37813.281 | Mean score: 779.0279329608938 | Max score: 8865.0\n",
      "Epoch: 180/1,001 | Loss value: 61131.594 | Mean score: 776.05 | Max score: 8865.0\n",
      "Epoch: 181/1,001 | Loss value: 18745.221 | Mean score: 773.5524861878453 | Max score: 8865.0\n",
      "Epoch: 182/1,001 | Loss value: 65091.621 | Mean score: 770.7857142857143 | Max score: 8865.0\n",
      "Epoch: 183/1,001 | Loss value: 53830.586 | Mean score: 766.8688524590164 | Max score: 8865.0\n",
      "Epoch: 184/1,001 | Loss value: 5570.790 | Mean score: 763.875 | Max score: 8865.0\n",
      "Epoch: 185/1,001 | Loss value: 1687.400 | Mean score: 763.2486486486487 | Max score: 8865.0\n",
      "Epoch: 186/1,001 | Loss value: 32033.672 | Mean score: 760.8387096774194 | Max score: 8865.0\n",
      "Epoch: 187/1,001 | Loss value: 136895.984 | Mean score: 759.9946524064171 | Max score: 8865.0\n",
      "Epoch: 188/1,001 | Loss value: 67623.047 | Mean score: 757.8191489361702 | Max score: 8865.0\n",
      "Epoch: 189/1,001 | Loss value: 8710.639 | Mean score: 756.7142857142857 | Max score: 8865.0\n",
      "Epoch: 190/1,001 | Loss value: 66413.930 | Mean score: 753.9157894736842 | Max score: 8865.0\n",
      "Epoch: 191/1,001 | Loss value: 28376.902 | Mean score: 750.5340314136125 | Max score: 8865.0\n",
      "Epoch: 192/1,001 | Loss value: 95402.148 | Mean score: 747.1875 | Max score: 8865.0\n",
      "Epoch: 193/1,001 | Loss value: 9683.006 | Mean score: 746.160621761658 | Max score: 8865.0\n",
      "Epoch: 194/1,001 | Loss value: 20521.547 | Mean score: 744.3092783505155 | Max score: 8865.0\n",
      "Epoch: 195/1,001 | Loss value: 4026.628 | Mean score: 742.2461538461539 | Max score: 8865.0\n",
      "Epoch: 196/1,001 | Loss value: 2008.715 | Mean score: 746.9081632653061 | Max score: 8865.0\n",
      "Epoch: 197/1,001 | Loss value: 12881.688 | Mean score: 744.4873096446701 | Max score: 8865.0\n",
      "Epoch: 198/1,001 | Loss value: 15505.414 | Mean score: 741.8181818181819 | Max score: 8865.0\n",
      "Epoch: 199/1,001 | Loss value: 35019.523 | Mean score: 739.4924623115578 | Max score: 8865.0\n",
      "Epoch: 200/1,001 | Loss value: 30447.029 | Mean score: 736.875 | Max score: 8865.0\n",
      "Epoch: 201/1,001 | Loss value: 18141.367 | Mean score: 733.3432835820895 | Max score: 8865.0\n",
      "Epoch: 202/1,001 | Loss value: 2475.415 | Mean score: 731.3613861386139 | Max score: 8865.0\n",
      "Epoch: 203/1,001 | Loss value: 37785.215 | Mean score: 730.4630541871921 | Max score: 8865.0\n",
      "Epoch: 204/1,001 | Loss value: 91606.250 | Mean score: 728.4264705882352 | Max score: 8865.0\n",
      "Epoch: 205/1,001 | Loss value: 22994.414 | Mean score: 735.3219512195122 | Max score: 8865.0\n",
      "Epoch: 206/1,001 | Loss value: 10241.668 | Mean score: 733.8932038834952 | Max score: 8865.0\n",
      "Epoch: 207/1,001 | Loss value: 32262.857 | Mean score: 734.9565217391304 | Max score: 8865.0\n",
      "Epoch: 208/1,001 | Loss value: 633.563 | Mean score: 732.4182692307693 | Max score: 8865.0\n",
      "Epoch: 209/1,001 | Loss value: 21191.162 | Mean score: 730.9377990430622 | Max score: 8865.0\n",
      "Epoch: 210/1,001 | Loss value: 10154.082 | Mean score: 729.9 | Max score: 8865.0\n",
      "Epoch: 211/1,001 | Loss value: 20603.738 | Mean score: 727.5924170616114 | Max score: 8865.0\n",
      "Epoch: 212/1,001 | Loss value: 19716.195 | Mean score: 736.2169811320755 | Max score: 8865.0\n",
      "Epoch: 213/1,001 | Loss value: 27836.953 | Mean score: 734.2816901408451 | Max score: 8865.0\n",
      "Epoch: 214/1,001 | Loss value: 84910.812 | Mean score: 731.6915887850467 | Max score: 8865.0\n",
      "Epoch: 215/1,001 | Loss value: 12948.325 | Mean score: 730.2976744186046 | Max score: 8865.0\n",
      "Epoch: 216/1,001 | Loss value: 19979.195 | Mean score: 733.25 | Max score: 8865.0\n",
      "Epoch: 217/1,001 | Loss value: 21240.650 | Mean score: 733.9769585253456 | Max score: 8865.0\n",
      "Epoch: 218/1,001 | Loss value: 42373.180 | Mean score: 731.5183486238532 | Max score: 8865.0\n",
      "Epoch: 219/1,001 | Loss value: 30730.141 | Mean score: 731.4246575342465 | Max score: 8865.0\n",
      "Epoch: 220/1,001 | Loss value: 37628.754 | Mean score: 731.7409090909091 | Max score: 8865.0\n",
      "Epoch: 221/1,001 | Loss value: 18269.662 | Mean score: 729.9366515837104 | Max score: 8865.0\n",
      "Epoch: 222/1,001 | Loss value: 12744.603 | Mean score: 730.6216216216217 | Max score: 8865.0\n",
      "Epoch: 223/1,001 | Loss value: 15569.822 | Mean score: 731.2600896860987 | Max score: 8865.0\n",
      "Epoch: 224/1,001 | Loss value: 12469.897 | Mean score: 735.2276785714286 | Max score: 8865.0\n",
      "Epoch: 225/1,001 | Loss value: 25625.010 | Mean score: 734.6 | Max score: 8865.0\n",
      "Epoch: 226/1,001 | Loss value: 17347.883 | Mean score: 733.6194690265487 | Max score: 8865.0\n",
      "Epoch: 227/1,001 | Loss value: 33027.559 | Mean score: 734.5110132158591 | Max score: 8865.0\n",
      "Epoch: 228/1,001 | Loss value: 28495.977 | Mean score: 736.7763157894736 | Max score: 8865.0\n",
      "Epoch: 229/1,001 | Loss value: 31175.387 | Mean score: 745.5851528384279 | Max score: 8865.0\n",
      "Epoch: 230/1,001 | Loss value: 29485.914 | Mean score: 746.3347826086956 | Max score: 8865.0\n",
      "Epoch: 231/1,001 | Loss value: 29894.762 | Mean score: 744.038961038961 | Max score: 8865.0\n",
      "Epoch: 232/1,001 | Loss value: 56664.129 | Mean score: 750.1034482758621 | Max score: 8865.0\n",
      "Epoch: 233/1,001 | Loss value: 31977.033 | Mean score: 747.4248927038626 | Max score: 8865.0\n",
      "Epoch: 234/1,001 | Loss value: 40433.488 | Mean score: 746.1153846153846 | Max score: 8865.0\n",
      "Epoch: 235/1,001 | Loss value: 110487.906 | Mean score: 754.2 | Max score: 8865.0\n",
      "Epoch: 236/1,001 | Loss value: 23283.197 | Mean score: 752.4533898305085 | Max score: 8865.0\n",
      "Epoch: 237/1,001 | Loss value: 132187.719 | Mean score: 750.7974683544304 | Max score: 8865.0\n",
      "Epoch: 238/1,001 | Loss value: 21706.801 | Mean score: 749.8739495798319 | Max score: 8865.0\n",
      "Epoch: 239/1,001 | Loss value: 199932.312 | Mean score: 749.4100418410042 | Max score: 8865.0\n",
      "Epoch: 240/1,001 | Loss value: 78288.797 | Mean score: 749.4 | Max score: 8865.0\n",
      "Epoch: 241/1,001 | Loss value: 232426.781 | Mean score: 747.6721991701245 | Max score: 8865.0\n",
      "Epoch: 242/1,001 | Loss value: 51901.590 | Mean score: 748.0041322314049 | Max score: 8865.0\n",
      "Epoch: 243/1,001 | Loss value: 40775.609 | Mean score: 745.074074074074 | Max score: 8865.0\n",
      "Epoch: 244/1,001 | Loss value: 67085.758 | Mean score: 744.1598360655738 | Max score: 8865.0\n",
      "Epoch: 245/1,001 | Loss value: 293536.188 | Mean score: 741.4530612244898 | Max score: 8865.0\n",
      "Epoch: 246/1,001 | Loss value: 40152.430 | Mean score: 739.0243902439024 | Max score: 8865.0\n",
      "Epoch: 247/1,001 | Loss value: 216637.094 | Mean score: 739.0202429149798 | Max score: 8865.0\n",
      "Epoch: 248/1,001 | Loss value: 26647.295 | Mean score: 737.7459677419355 | Max score: 8865.0\n",
      "Epoch: 249/1,001 | Loss value: 265177.250 | Mean score: 737.4216867469879 | Max score: 8865.0\n",
      "Epoch: 250/1,001 | Loss value: 34899.723 | Mean score: 737.964 | Max score: 8865.0\n",
      "Epoch: 251/1,001 | Loss value: 378649.750 | Mean score: 749.2948207171315 | Max score: 8865.0\n",
      "Epoch: 252/1,001 | Loss value: 267724.812 | Mean score: 747.6785714285714 | Max score: 8865.0\n",
      "Epoch: 253/1,001 | Loss value: 632910.562 | Mean score: 746.2529644268775 | Max score: 8865.0\n",
      "Epoch: 254/1,001 | Loss value: 48112.371 | Mean score: 745.2637795275591 | Max score: 8865.0\n",
      "Epoch: 255/1,001 | Loss value: 71238.250 | Mean score: 745.8705882352941 | Max score: 8865.0\n",
      "Epoch: 256/1,001 | Loss value: 82818.891 | Mean score: 743.4140625 | Max score: 8865.0\n",
      "Epoch: 257/1,001 | Loss value: 120716.109 | Mean score: 741.5369649805448 | Max score: 8865.0\n",
      "Epoch: 258/1,001 | Loss value: 137388.188 | Mean score: 740.3372093023256 | Max score: 8865.0\n",
      "Epoch: 259/1,001 | Loss value: 210727.500 | Mean score: 741.8918918918919 | Max score: 8865.0\n",
      "Epoch: 260/1,001 | Loss value: 236164.797 | Mean score: 742.1192307692307 | Max score: 8865.0\n",
      "Epoch: 261/1,001 | Loss value: 118738.859 | Mean score: 743.0344827586207 | Max score: 8865.0\n",
      "Epoch: 262/1,001 | Loss value: 215621.156 | Mean score: 741.263358778626 | Max score: 8865.0\n",
      "Epoch: 263/1,001 | Loss value: 171379.062 | Mean score: 740.0190114068441 | Max score: 8865.0\n",
      "Epoch: 264/1,001 | Loss value: 77733.742 | Mean score: 737.6931818181819 | Max score: 8865.0\n",
      "Epoch: 265/1,001 | Loss value: 87026.812 | Mean score: 736.0981132075472 | Max score: 8865.0\n",
      "Epoch: 266/1,001 | Loss value: 143000.359 | Mean score: 734.2105263157895 | Max score: 8865.0\n",
      "Epoch: 267/1,001 | Loss value: 140552.469 | Mean score: 733.4494382022472 | Max score: 8865.0\n",
      "Epoch: 268/1,001 | Loss value: 6360.425 | Mean score: 731.7201492537314 | Max score: 8865.0\n",
      "Epoch: 269/1,001 | Loss value: 69779.023 | Mean score: 731.8104089219331 | Max score: 8865.0\n",
      "Epoch: 270/1,001 | Loss value: 33649.758 | Mean score: 730.3 | Max score: 8865.0\n",
      "Epoch: 271/1,001 | Loss value: 32040.775 | Mean score: 728.269372693727 | Max score: 8865.0\n",
      "Epoch: 272/1,001 | Loss value: 70985.758 | Mean score: 726.1213235294117 | Max score: 8865.0\n",
      "Epoch: 273/1,001 | Loss value: 237320.938 | Mean score: 728.1098901098901 | Max score: 8865.0\n",
      "Epoch: 274/1,001 | Loss value: 313726.469 | Mean score: 726.2737226277372 | Max score: 8865.0\n",
      "Epoch: 275/1,001 | Loss value: 41063.727 | Mean score: 724.3527272727273 | Max score: 8865.0\n",
      "Epoch: 276/1,001 | Loss value: 63129.477 | Mean score: 722.5108695652174 | Max score: 8865.0\n",
      "Epoch: 277/1,001 | Loss value: 36928.219 | Mean score: 724.5487364620939 | Max score: 8865.0\n",
      "Epoch: 278/1,001 | Loss value: 30954.920 | Mean score: 729.5179856115108 | Max score: 8865.0\n",
      "Epoch: 279/1,001 | Loss value: 80220.844 | Mean score: 727.1612903225806 | Max score: 8865.0\n",
      "Epoch: 280/1,001 | Loss value: 38963.809 | Mean score: 727.7142857142857 | Max score: 8865.0\n",
      "Epoch: 281/1,001 | Loss value: 88638.422 | Mean score: 727.3024911032029 | Max score: 8865.0\n",
      "Epoch: 282/1,001 | Loss value: 81852.250 | Mean score: 726.0 | Max score: 8865.0\n",
      "Epoch: 283/1,001 | Loss value: 82679.867 | Mean score: 723.5936395759717 | Max score: 8865.0\n",
      "Epoch: 284/1,001 | Loss value: 28459.107 | Mean score: 721.9014084507043 | Max score: 8865.0\n",
      "Epoch: 285/1,001 | Loss value: 19919.996 | Mean score: 721.8315789473684 | Max score: 8865.0\n",
      "Epoch: 286/1,001 | Loss value: 168410.766 | Mean score: 722.1398601398602 | Max score: 8865.0\n",
      "Epoch: 287/1,001 | Loss value: 264229.625 | Mean score: 719.8432055749129 | Max score: 8865.0\n",
      "Epoch: 288/1,001 | Loss value: 151755.234 | Mean score: 720.53125 | Max score: 8865.0\n",
      "Epoch: 289/1,001 | Loss value: 100791.523 | Mean score: 734.6678200692041 | Max score: 8865.0\n",
      "Epoch: 290/1,001 | Loss value: 413546.844 | Mean score: 732.6931034482759 | Max score: 8865.0\n",
      "Epoch: 291/1,001 | Loss value: 17425.893 | Mean score: 730.7938144329897 | Max score: 8865.0\n",
      "Epoch: 292/1,001 | Loss value: 383215.438 | Mean score: 728.9691780821918 | Max score: 8865.0\n",
      "Epoch: 293/1,001 | Loss value: 36495.336 | Mean score: 731.0887372013652 | Max score: 8865.0\n",
      "Epoch: 294/1,001 | Loss value: 19496.721 | Mean score: 730.8979591836735 | Max score: 8865.0\n",
      "Epoch: 295/1,001 | Loss value: 74588.383 | Mean score: 729.3661016949153 | Max score: 8865.0\n",
      "Epoch: 296/1,001 | Loss value: 3179874.500 | Mean score: 728.9391891891892 | Max score: 8865.0\n",
      "Epoch: 297/1,001 | Loss value: 178594.500 | Mean score: 727.4242424242424 | Max score: 8865.0\n",
      "Epoch: 298/1,001 | Loss value: 159331.547 | Mean score: 727.3691275167786 | Max score: 8865.0\n",
      "Epoch: 299/1,001 | Loss value: 254002.141 | Mean score: 726.2608695652174 | Max score: 8865.0\n",
      "Epoch: 300/1,001 | Loss value: 117384.609 | Mean score: 725.61 | Max score: 8865.0\n",
      "Epoch: 301/1,001 | Loss value: 180155.234 | Mean score: 725.6511627906976 | Max score: 8865.0\n",
      "Epoch: 302/1,001 | Loss value: 22457.744 | Mean score: 725.4834437086092 | Max score: 8865.0\n",
      "Epoch: 303/1,001 | Loss value: 344914.844 | Mean score: 725.6138613861386 | Max score: 8865.0\n",
      "Epoch: 304/1,001 | Loss value: 422621.625 | Mean score: 729.0296052631579 | Max score: 8865.0\n",
      "Epoch: 305/1,001 | Loss value: 185351.766 | Mean score: 734.4295081967213 | Max score: 8865.0\n",
      "Epoch: 306/1,001 | Loss value: 29370.555 | Mean score: 733.0882352941177 | Max score: 8865.0\n",
      "Epoch: 307/1,001 | Loss value: 234808.797 | Mean score: 731.257328990228 | Max score: 8865.0\n",
      "Epoch: 308/1,001 | Loss value: 273281.594 | Mean score: 729.525974025974 | Max score: 8865.0\n",
      "Epoch: 309/1,001 | Loss value: 38906.051 | Mean score: 728.7669902912621 | Max score: 8865.0\n",
      "Epoch: 310/1,001 | Loss value: 34636.895 | Mean score: 727.8967741935484 | Max score: 8865.0\n",
      "Epoch: 311/1,001 | Loss value: 86343.078 | Mean score: 822.7909967845659 | Max score: 30240.0\n",
      "Epoch: 312/1,001 | Loss value: 500909.250 | Mean score: 826.0096153846154 | Max score: 30240.0\n",
      "Epoch: 313/1,001 | Loss value: 68578.508 | Mean score: 824.6645367412141 | Max score: 30240.0\n",
      "Epoch: 314/1,001 | Loss value: 104203.438 | Mean score: 824.875796178344 | Max score: 30240.0\n",
      "Epoch: 315/1,001 | Loss value: 336963.125 | Mean score: 826.6571428571428 | Max score: 30240.0\n",
      "Epoch: 316/1,001 | Loss value: 70900.562 | Mean score: 824.8386075949367 | Max score: 30240.0\n",
      "Epoch: 317/1,001 | Loss value: 151980.781 | Mean score: 829.0788643533123 | Max score: 30240.0\n",
      "Epoch: 318/1,001 | Loss value: 62004.891 | Mean score: 826.8679245283018 | Max score: 30240.0\n",
      "Epoch: 319/1,001 | Loss value: 65044.891 | Mean score: 825.7429467084639 | Max score: 30240.0\n",
      "Epoch: 320/1,001 | Loss value: 89154.344 | Mean score: 824.68125 | Max score: 30240.0\n",
      "Epoch: 321/1,001 | Loss value: 210688.516 | Mean score: 823.0093457943925 | Max score: 30240.0\n",
      "Epoch: 322/1,001 | Loss value: 100433.398 | Mean score: 823.276397515528 | Max score: 30240.0\n",
      "Epoch: 323/1,001 | Loss value: 339442.094 | Mean score: 824.5448916408669 | Max score: 30240.0\n",
      "Epoch: 324/1,001 | Loss value: 43869.250 | Mean score: 823.25 | Max score: 30240.0\n",
      "Epoch: 325/1,001 | Loss value: 326233.375 | Mean score: 826.5323076923077 | Max score: 30240.0\n",
      "Epoch: 326/1,001 | Loss value: 429102.438 | Mean score: 824.4938650306749 | Max score: 30240.0\n",
      "Epoch: 327/1,001 | Loss value: 317382.375 | Mean score: 822.2752293577981 | Max score: 30240.0\n",
      "Epoch: 328/1,001 | Loss value: 47161.012 | Mean score: 820.234756097561 | Max score: 30240.0\n",
      "Epoch: 329/1,001 | Loss value: 110368.398 | Mean score: 818.644376899696 | Max score: 30240.0\n",
      "Epoch: 330/1,001 | Loss value: 1059477.000 | Mean score: 816.5727272727273 | Max score: 30240.0\n",
      "Epoch: 331/1,001 | Loss value: 182014.078 | Mean score: 814.595166163142 | Max score: 30240.0\n",
      "Epoch: 332/1,001 | Loss value: 26342.285 | Mean score: 812.3042168674699 | Max score: 30240.0\n",
      "Epoch: 333/1,001 | Loss value: 45394.316 | Mean score: 810.6486486486486 | Max score: 30240.0\n",
      "Epoch: 334/1,001 | Loss value: 60867.359 | Mean score: 813.3413173652694 | Max score: 30240.0\n",
      "Epoch: 335/1,001 | Loss value: 968245.250 | Mean score: 811.5313432835821 | Max score: 30240.0\n",
      "Epoch: 336/1,001 | Loss value: 737984.500 | Mean score: 811.9821428571429 | Max score: 30240.0\n",
      "Epoch: 337/1,001 | Loss value: 888083.125 | Mean score: 809.919881305638 | Max score: 30240.0\n",
      "Epoch: 338/1,001 | Loss value: 1372789.500 | Mean score: 813.008875739645 | Max score: 30240.0\n",
      "Epoch: 339/1,001 | Loss value: 280754.875 | Mean score: 812.3097345132743 | Max score: 30240.0\n",
      "Epoch: 340/1,001 | Loss value: 99943.797 | Mean score: 810.714705882353 | Max score: 30240.0\n",
      "Epoch: 341/1,001 | Loss value: 21874.234 | Mean score: 811.0029325513196 | Max score: 30240.0\n",
      "Epoch: 342/1,001 | Loss value: 39582.504 | Mean score: 815.3947368421053 | Max score: 30240.0\n",
      "Epoch: 343/1,001 | Loss value: 67475.484 | Mean score: 813.1224489795918 | Max score: 30240.0\n",
      "Epoch: 344/1,001 | Loss value: 107254.383 | Mean score: 811.1773255813954 | Max score: 30240.0\n",
      "Epoch: 345/1,001 | Loss value: 371257.812 | Mean score: 811.7739130434783 | Max score: 30240.0\n",
      "Epoch: 346/1,001 | Loss value: 3844560.250 | Mean score: 810.1300578034682 | Max score: 30240.0\n",
      "Epoch: 347/1,001 | Loss value: 214102.641 | Mean score: 809.014409221902 | Max score: 30240.0\n",
      "Epoch: 348/1,001 | Loss value: 2589130.000 | Mean score: 810.4396551724138 | Max score: 30240.0\n",
      "Epoch: 349/1,001 | Loss value: 69902.188 | Mean score: 812.6561604584527 | Max score: 30240.0\n",
      "Epoch: 350/1,001 | Loss value: 33015.520 | Mean score: 812.3657142857143 | Max score: 30240.0\n",
      "Epoch: 351/1,001 | Loss value: 47713.184 | Mean score: 810.5384615384615 | Max score: 30240.0\n",
      "Epoch: 352/1,001 | Loss value: 57394.898 | Mean score: 811.2784090909091 | Max score: 30240.0\n",
      "Epoch: 353/1,001 | Loss value: 390216.844 | Mean score: 809.5665722379604 | Max score: 30240.0\n",
      "Epoch: 354/1,001 | Loss value: 43002.301 | Mean score: 808.8050847457627 | Max score: 30240.0\n",
      "Epoch: 355/1,001 | Loss value: 454234.250 | Mean score: 807.312676056338 | Max score: 30240.0\n",
      "Epoch: 356/1,001 | Loss value: 327090.812 | Mean score: 808.0028089887641 | Max score: 30240.0\n",
      "Epoch: 357/1,001 | Loss value: 56864.750 | Mean score: 806.1176470588235 | Max score: 30240.0\n",
      "Epoch: 358/1,001 | Loss value: 76450.547 | Mean score: 804.6703910614525 | Max score: 30240.0\n",
      "Epoch: 359/1,001 | Loss value: 134101.141 | Mean score: 804.259052924791 | Max score: 30240.0\n",
      "Epoch: 360/1,001 | Loss value: 178389.719 | Mean score: 802.775 | Max score: 30240.0\n",
      "Epoch: 361/1,001 | Loss value: 1237436.125 | Mean score: 800.7506925207756 | Max score: 30240.0\n",
      "Epoch: 362/1,001 | Loss value: 171406.484 | Mean score: 802.6160220994475 | Max score: 30240.0\n",
      "Epoch: 363/1,001 | Loss value: 4067.724 | Mean score: 802.1157024793389 | Max score: 30240.0\n",
      "Epoch: 364/1,001 | Loss value: 46845.594 | Mean score: 801.1483516483516 | Max score: 30240.0\n",
      "Epoch: 365/1,001 | Loss value: 1656180.750 | Mean score: 808.3972602739726 | Max score: 30240.0\n",
      "Epoch: 366/1,001 | Loss value: 386858.281 | Mean score: 806.9262295081967 | Max score: 30240.0\n",
      "Epoch: 367/1,001 | Loss value: 660526.438 | Mean score: 804.9972752043597 | Max score: 30240.0\n",
      "Epoch: 368/1,001 | Loss value: 470969.188 | Mean score: 805.6467391304348 | Max score: 30240.0\n",
      "Epoch: 369/1,001 | Loss value: 234992.281 | Mean score: 803.780487804878 | Max score: 30240.0\n",
      "Epoch: 370/1,001 | Loss value: 770566.750 | Mean score: 805.5 | Max score: 30240.0\n",
      "Epoch: 371/1,001 | Loss value: 81122.289 | Mean score: 804.9056603773585 | Max score: 30240.0\n",
      "Epoch: 372/1,001 | Loss value: 204319.906 | Mean score: 803.25 | Max score: 30240.0\n",
      "Epoch: 373/1,001 | Loss value: 28461.920 | Mean score: 801.7479892761394 | Max score: 30240.0\n",
      "Epoch: 374/1,001 | Loss value: 333892.469 | Mean score: 802.2994652406417 | Max score: 30240.0\n",
      "Epoch: 375/1,001 | Loss value: 343039.469 | Mean score: 802.032 | Max score: 30240.0\n",
      "Epoch: 376/1,001 | Loss value: 624059.562 | Mean score: 800.4015957446809 | Max score: 30240.0\n",
      "Epoch: 377/1,001 | Loss value: 51801.539 | Mean score: 798.8514588859416 | Max score: 30240.0\n",
      "Epoch: 378/1,001 | Loss value: 193047.875 | Mean score: 798.8333333333334 | Max score: 30240.0\n",
      "Epoch: 379/1,001 | Loss value: 86604.180 | Mean score: 798.910290237467 | Max score: 30240.0\n",
      "Epoch: 380/1,001 | Loss value: 162766.438 | Mean score: 797.9921052631579 | Max score: 30240.0\n",
      "Epoch: 381/1,001 | Loss value: 72807.102 | Mean score: 796.4409448818898 | Max score: 30240.0\n",
      "Epoch: 382/1,001 | Loss value: 29926.520 | Mean score: 795.8403141361257 | Max score: 30240.0\n",
      "Epoch: 383/1,001 | Loss value: 497566.844 | Mean score: 797.0522193211489 | Max score: 30240.0\n",
      "Epoch: 384/1,001 | Loss value: 159646.656 | Mean score: 796.7109375 | Max score: 30240.0\n",
      "Epoch: 385/1,001 | Loss value: 1891060.375 | Mean score: 796.0675324675325 | Max score: 30240.0\n",
      "Epoch: 386/1,001 | Loss value: 52362.270 | Mean score: 796.080310880829 | Max score: 30240.0\n",
      "Epoch: 387/1,001 | Loss value: 428442.562 | Mean score: 795.3488372093024 | Max score: 30240.0\n",
      "Epoch: 388/1,001 | Loss value: 411371.594 | Mean score: 798.1469072164948 | Max score: 30240.0\n",
      "Epoch: 389/1,001 | Loss value: 108051.094 | Mean score: 797.6915167095116 | Max score: 30240.0\n",
      "Epoch: 390/1,001 | Loss value: 112532.812 | Mean score: 797.4692307692308 | Max score: 30240.0\n",
      "Epoch: 391/1,001 | Loss value: 196137.359 | Mean score: 796.0741687979539 | Max score: 30240.0\n",
      "Epoch: 392/1,001 | Loss value: 142138.109 | Mean score: 794.7551020408164 | Max score: 30240.0\n",
      "Epoch: 393/1,001 | Loss value: 143619.000 | Mean score: 794.2213740458016 | Max score: 30240.0\n",
      "Epoch: 394/1,001 | Loss value: 782135.812 | Mean score: 794.7182741116751 | Max score: 30240.0\n",
      "Epoch: 395/1,001 | Loss value: 315868.688 | Mean score: 796.1924050632912 | Max score: 30240.0\n",
      "Epoch: 396/1,001 | Loss value: 237108.812 | Mean score: 917.3409090909091 | Max score: 48771.0\n",
      "Epoch: 397/1,001 | Loss value: 29279.109 | Mean score: 916.639798488665 | Max score: 48771.0\n",
      "Epoch: 398/1,001 | Loss value: 31906.994 | Mean score: 915.7613065326633 | Max score: 48771.0\n",
      "Epoch: 399/1,001 | Loss value: 142031.016 | Mean score: 914.187969924812 | Max score: 48771.0\n",
      "Epoch: 400/1,001 | Loss value: 301456.469 | Mean score: 911.9925 | Max score: 48771.0\n",
      "Epoch: 401/1,001 | Loss value: 48780.734 | Mean score: 910.4364089775561 | Max score: 48771.0\n",
      "Epoch: 402/1,001 | Loss value: 558310.688 | Mean score: 908.4850746268656 | Max score: 48771.0\n",
      "Epoch: 403/1,001 | Loss value: 203052.922 | Mean score: 907.1910669975186 | Max score: 48771.0\n",
      "Epoch: 404/1,001 | Loss value: 353559.469 | Mean score: 908.019801980198 | Max score: 48771.0\n",
      "Epoch: 405/1,001 | Loss value: 184826.844 | Mean score: 906.1777777777778 | Max score: 48771.0\n",
      "Epoch: 406/1,001 | Loss value: 542643.625 | Mean score: 904.9211822660099 | Max score: 48771.0\n",
      "Epoch: 407/1,001 | Loss value: 31710.922 | Mean score: 903.4938574938575 | Max score: 48771.0\n",
      "Epoch: 408/1,001 | Loss value: 113020.891 | Mean score: 901.8970588235294 | Max score: 48771.0\n",
      "Epoch: 409/1,001 | Loss value: 201918.844 | Mean score: 905.5672371638142 | Max score: 48771.0\n",
      "Epoch: 410/1,001 | Loss value: 605505.312 | Mean score: 904.1048780487805 | Max score: 48771.0\n",
      "Epoch: 411/1,001 | Loss value: 828108.625 | Mean score: 902.4963503649635 | Max score: 48771.0\n",
      "Epoch: 412/1,001 | Loss value: 179925.516 | Mean score: 901.1577669902913 | Max score: 48771.0\n",
      "Epoch: 413/1,001 | Loss value: 41938.355 | Mean score: 901.5254237288135 | Max score: 48771.0\n",
      "Epoch: 414/1,001 | Loss value: 342710.812 | Mean score: 900.2608695652174 | Max score: 48771.0\n",
      "Epoch: 415/1,001 | Loss value: 81606.352 | Mean score: 907.3951807228916 | Max score: 48771.0\n",
      "Epoch: 416/1,001 | Loss value: 345860.094 | Mean score: 905.6682692307693 | Max score: 48771.0\n",
      "Epoch: 417/1,001 | Loss value: 410016.844 | Mean score: 904.6187050359712 | Max score: 48771.0\n",
      "Epoch: 418/1,001 | Loss value: 290074.906 | Mean score: 902.6913875598086 | Max score: 48771.0\n",
      "Epoch: 419/1,001 | Loss value: 51859.090 | Mean score: 900.7517899761336 | Max score: 48771.0\n",
      "Epoch: 420/1,001 | Loss value: 160427.719 | Mean score: 899.1428571428571 | Max score: 48771.0\n",
      "Epoch: 421/1,001 | Loss value: 131044.211 | Mean score: 897.9904988123516 | Max score: 48771.0\n",
      "Epoch: 422/1,001 | Loss value: 81881.312 | Mean score: 896.4383886255924 | Max score: 48771.0\n",
      "Epoch: 423/1,001 | Loss value: 352506.438 | Mean score: 895.0 | Max score: 48771.0\n",
      "Epoch: 424/1,001 | Loss value: 564181.812 | Mean score: 894.7358490566038 | Max score: 48771.0\n",
      "Epoch: 425/1,001 | Loss value: 27529.029 | Mean score: 894.8117647058823 | Max score: 48771.0\n",
      "Epoch: 426/1,001 | Loss value: 35101.727 | Mean score: 894.6549295774648 | Max score: 48771.0\n",
      "Epoch: 427/1,001 | Loss value: 155644.984 | Mean score: 900.695550351288 | Max score: 48771.0\n",
      "Epoch: 428/1,001 | Loss value: 413019.406 | Mean score: 899.5373831775701 | Max score: 48771.0\n",
      "Epoch: 429/1,001 | Loss value: 33513.188 | Mean score: 899.0769230769231 | Max score: 48771.0\n",
      "Epoch: 430/1,001 | Loss value: 307706.375 | Mean score: 897.5302325581396 | Max score: 48771.0\n",
      "Epoch: 431/1,001 | Loss value: 232317.984 | Mean score: 896.0324825986079 | Max score: 48771.0\n",
      "Epoch: 432/1,001 | Loss value: 310099.438 | Mean score: 895.1875 | Max score: 48771.0\n",
      "Epoch: 433/1,001 | Loss value: 56384004.000 | Mean score: 896.8198614318707 | Max score: 48771.0\n",
      "Epoch: 434/1,001 | Loss value: 844433.688 | Mean score: 895.4585253456221 | Max score: 48771.0\n",
      "Epoch: 435/1,001 | Loss value: 199254.641 | Mean score: 893.9793103448276 | Max score: 48771.0\n",
      "Epoch: 436/1,001 | Loss value: 2660696.500 | Mean score: 892.3830275229358 | Max score: 48771.0\n",
      "Epoch: 437/1,001 | Loss value: 170972.359 | Mean score: 892.9359267734553 | Max score: 48771.0\n",
      "Epoch: 438/1,001 | Loss value: 169489.922 | Mean score: 891.9041095890411 | Max score: 48771.0\n",
      "Epoch: 439/1,001 | Loss value: 44374.570 | Mean score: 891.7790432801822 | Max score: 48771.0\n",
      "Epoch: 440/1,001 | Loss value: 216668.141 | Mean score: 891.7363636363636 | Max score: 48771.0\n",
      "Epoch: 441/1,001 | Loss value: 682688.375 | Mean score: 889.9591836734694 | Max score: 48771.0\n",
      "Epoch: 442/1,001 | Loss value: 128301.188 | Mean score: 895.4592760180996 | Max score: 48771.0\n",
      "Epoch: 443/1,001 | Loss value: 107412.312 | Mean score: 893.5598194130926 | Max score: 48771.0\n",
      "Epoch: 444/1,001 | Loss value: 72858.039 | Mean score: 892.6824324324324 | Max score: 48771.0\n",
      "Epoch: 445/1,001 | Loss value: 284873.844 | Mean score: 891.991011235955 | Max score: 48771.0\n",
      "Epoch: 446/1,001 | Loss value: 78406.727 | Mean score: 890.8385650224216 | Max score: 48771.0\n",
      "Epoch: 447/1,001 | Loss value: 293966.469 | Mean score: 890.3959731543624 | Max score: 48771.0\n",
      "Epoch: 448/1,001 | Loss value: 1439125.500 | Mean score: 891.3616071428571 | Max score: 48771.0\n",
      "Epoch: 449/1,001 | Loss value: 394858.594 | Mean score: 890.6592427616927 | Max score: 48771.0\n",
      "Epoch: 450/1,001 | Loss value: 81802.367 | Mean score: 889.18 | Max score: 48771.0\n",
      "Epoch: 451/1,001 | Loss value: 637124.875 | Mean score: 888.4855875831486 | Max score: 48771.0\n",
      "Epoch: 452/1,001 | Loss value: 75368.945 | Mean score: 887.2765486725664 | Max score: 48771.0\n",
      "Epoch: 453/1,001 | Loss value: 448485.281 | Mean score: 886.5695364238411 | Max score: 48771.0\n",
      "Epoch: 454/1,001 | Loss value: 859339.125 | Mean score: 885.2312775330397 | Max score: 48771.0\n",
      "Epoch: 455/1,001 | Loss value: 110001.523 | Mean score: 884.7494505494506 | Max score: 48771.0\n",
      "Epoch: 456/1,001 | Loss value: 266148.812 | Mean score: 883.0657894736842 | Max score: 48771.0\n",
      "Epoch: 457/1,001 | Loss value: 53382.676 | Mean score: 882.3544857768053 | Max score: 48771.0\n",
      "Epoch: 458/1,001 | Loss value: 150401.172 | Mean score: 882.2947598253276 | Max score: 48771.0\n",
      "Epoch: 459/1,001 | Loss value: 51667.984 | Mean score: 882.4509803921569 | Max score: 48771.0\n",
      "Epoch: 460/1,001 | Loss value: 609312.625 | Mean score: 882.2934782608696 | Max score: 48771.0\n",
      "Epoch: 461/1,001 | Loss value: 137813.844 | Mean score: 880.9848156182212 | Max score: 48771.0\n",
      "Epoch: 462/1,001 | Loss value: 10335.856 | Mean score: 881.1428571428571 | Max score: 48771.0\n",
      "Epoch: 463/1,001 | Loss value: 121913.820 | Mean score: 882.4470842332613 | Max score: 48771.0\n",
      "Epoch: 464/1,001 | Loss value: 451201.125 | Mean score: 880.9525862068965 | Max score: 48771.0\n",
      "Epoch: 465/1,001 | Loss value: 90825.203 | Mean score: 879.3483870967742 | Max score: 48771.0\n",
      "Epoch: 466/1,001 | Loss value: 233165.797 | Mean score: 883.4871244635193 | Max score: 48771.0\n",
      "Epoch: 467/1,001 | Loss value: 72445.680 | Mean score: 882.2312633832977 | Max score: 48771.0\n",
      "Epoch: 468/1,001 | Loss value: 53601.895 | Mean score: 880.6923076923077 | Max score: 48771.0\n",
      "Epoch: 469/1,001 | Loss value: 244654.688 | Mean score: 879.4285714285714 | Max score: 48771.0\n",
      "Epoch: 470/1,001 | Loss value: 137650.688 | Mean score: 878.3808510638298 | Max score: 48771.0\n",
      "Epoch: 471/1,001 | Loss value: 138196.453 | Mean score: 877.2229299363057 | Max score: 48771.0\n",
      "Epoch: 472/1,001 | Loss value: 408595.406 | Mean score: 876.375 | Max score: 48771.0\n",
      "Epoch: 473/1,001 | Loss value: 69890.906 | Mean score: 875.892177589852 | Max score: 48771.0\n",
      "Epoch: 474/1,001 | Loss value: 301688.906 | Mean score: 874.5949367088608 | Max score: 48771.0\n",
      "Epoch: 475/1,001 | Loss value: 156501.109 | Mean score: 876.0315789473684 | Max score: 48771.0\n",
      "Epoch: 476/1,001 | Loss value: 106534.523 | Mean score: 874.5504201680673 | Max score: 48771.0\n",
      "Epoch: 477/1,001 | Loss value: 403532.969 | Mean score: 876.2452830188679 | Max score: 48771.0\n",
      "Epoch: 478/1,001 | Loss value: 906095.438 | Mean score: 874.6380753138076 | Max score: 48771.0\n",
      "Epoch: 479/1,001 | Loss value: 241367.484 | Mean score: 873.4321503131524 | Max score: 48771.0\n",
      "Epoch: 480/1,001 | Loss value: 102649.523 | Mean score: 872.3625 | Max score: 48771.0\n",
      "Epoch: 481/1,001 | Loss value: 149797.938 | Mean score: 871.0353430353431 | Max score: 48771.0\n",
      "Epoch: 482/1,001 | Loss value: 400866.719 | Mean score: 870.4605809128631 | Max score: 48771.0\n",
      "Epoch: 483/1,001 | Loss value: 51807.430 | Mean score: 868.8447204968944 | Max score: 48771.0\n",
      "Epoch: 484/1,001 | Loss value: 39991.344 | Mean score: 869.1136363636364 | Max score: 48771.0\n",
      "Epoch: 485/1,001 | Loss value: 464596.156 | Mean score: 867.5628865979381 | Max score: 48771.0\n",
      "Epoch: 486/1,001 | Loss value: 170801.781 | Mean score: 870.6296296296297 | Max score: 48771.0\n",
      "Epoch: 487/1,001 | Loss value: 453266.031 | Mean score: 869.0636550308009 | Max score: 48771.0\n",
      "Epoch: 488/1,001 | Loss value: 242120.203 | Mean score: 870.7131147540983 | Max score: 48771.0\n",
      "Epoch: 489/1,001 | Loss value: 203584.594 | Mean score: 869.3374233128834 | Max score: 48771.0\n",
      "Epoch: 490/1,001 | Loss value: 90117.148 | Mean score: 868.0408163265306 | Max score: 48771.0\n",
      "Epoch: 491/1,001 | Loss value: 2485556.000 | Mean score: 866.5845213849287 | Max score: 48771.0\n",
      "Epoch: 492/1,001 | Loss value: 36305.219 | Mean score: 865.5182926829268 | Max score: 48771.0\n",
      "Epoch: 493/1,001 | Loss value: 572332.562 | Mean score: 865.0953346855983 | Max score: 48771.0\n",
      "Epoch: 494/1,001 | Loss value: 1199684.625 | Mean score: 865.5485829959514 | Max score: 48771.0\n",
      "Epoch: 495/1,001 | Loss value: 17962.738 | Mean score: 865.1090909090909 | Max score: 48771.0\n",
      "Epoch: 496/1,001 | Loss value: 115569.383 | Mean score: 863.6008064516129 | Max score: 48771.0\n",
      "Epoch: 497/1,001 | Loss value: 1048695.250 | Mean score: 862.7867203219316 | Max score: 48771.0\n",
      "Epoch: 498/1,001 | Loss value: 171487.312 | Mean score: 861.6506024096385 | Max score: 48771.0\n",
      "Epoch: 499/1,001 | Loss value: 1898761.000 | Mean score: 861.438877755511 | Max score: 48771.0\n",
      "Epoch: 500/1,001 | Loss value: 227707.078 | Mean score: 861.21 | Max score: 48771.0\n",
      "Epoch: 501/1,001 | Loss value: 1123456.000 | Mean score: 860.8023952095808 | Max score: 48771.0\n",
      "Epoch: 502/1,001 | Loss value: 296501.594 | Mean score: 860.3964143426294 | Max score: 48771.0\n",
      "Epoch: 503/1,001 | Loss value: 87875.539 | Mean score: 859.0079522862823 | Max score: 48771.0\n",
      "Epoch: 504/1,001 | Loss value: 156644.188 | Mean score: 857.5357142857143 | Max score: 48771.0\n",
      "Epoch: 505/1,001 | Loss value: 156527.234 | Mean score: 859.8831683168316 | Max score: 48771.0\n",
      "Epoch: 506/1,001 | Loss value: 860939.125 | Mean score: 858.6462450592885 | Max score: 48771.0\n",
      "Epoch: 507/1,001 | Loss value: 125701.500 | Mean score: 858.1952662721893 | Max score: 48771.0\n"
     ]
    }
   ],
   "source": [
    "# Run Training\n",
    "\n",
    "# Define environment\n",
    "action_map = {0: 'a', 1: 'w', 2:'s', 3:'d'}\n",
    "key_list = list(action_map.keys())\n",
    "val_list = list(action_map.values())\n",
    "\n",
    "# Initialize experience replay object\n",
    "exp_replay = ExperienceReplay(max_memory=2048)\n",
    "\n",
    "# Exploration rate\n",
    "epsilon = .1  \n",
    "\n",
    "# Training variables\n",
    "n_episodes = 11\n",
    "results = []\n",
    "loss = float('inf')\n",
    "    \n",
    "for e in range(1, n_episodes): \n",
    "\n",
    "    if (e == 1) or (e % 10 == 1) :\n",
    "        print(f\"Epoch: {e:03d}/{n_episodes:,} | Loss value: {loss:>6.3f} | Mean score: {np.mean(results)} | Max score: {np.max(results)}\")\n",
    "        \n",
    "    # The next new episode.\n",
    "    game_threes = Threes()\n",
    " \n",
    "    # While the game is not over\n",
    "    while game_threes.playable():\n",
    "        \n",
    "        # Get initial input (as vector).\n",
    "        current_state = game_threes.state\n",
    "        allowed_actions =  allowed_moves(current_state)\n",
    "        \n",
    "        # Get next action - You guessed it eplison-greedy.\n",
    "        if np.random.rand() > epsilon:\n",
    "            q = model.predict(hash_num(current_state))\n",
    "            action = action_map[np.argmax(q[0])]\n",
    "        else:\n",
    "            action = np.random.choice(allowed_actions)\n",
    "        if action not in allowed_actions:\n",
    "            action = np.random.choice(allowed_actions)\n",
    "        \n",
    "        # Apply action, get rewards and new state.\n",
    "        next_state, reward = try_move(current_state, action)\n",
    "        game_threes.make_move(action)\n",
    "        action_num = key_list[val_list.index(action)]\n",
    "        \n",
    "        # Store experience.\n",
    "        exp_replay.remember([hash_num(current_state), action_num, reward, hash_num(next_state)], game_threes.playable())\n",
    "\n",
    "        # Get collected data to train model.\n",
    "        inputs, targets = exp_replay.get_batch(model, batch_size=50)\n",
    "\n",
    "        # Train model on experiences.\n",
    "        loss = model.train_on_batch(inputs, targets)\n",
    "        \n",
    "    results.append(get_score(game_threes.state))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-u5dGuZ_4fa2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "Env.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
