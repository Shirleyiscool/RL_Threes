{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T02:36:33.274965Z",
     "start_time": "2020-06-26T02:36:33.024461Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time \n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T02:36:33.923630Z",
     "start_time": "2020-06-26T02:36:33.892307Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "HCTO0iNG4n9V"
   },
   "outputs": [],
   "source": [
    "def movable_condition(first, second):\n",
    "    \"\"\"Define whether two close tile can be merged\"\"\"\n",
    "    return ((first == 0) and (second != 0)) or \\\n",
    "           ((np.any(np.array([first, second]) > 2)) and (first == second)) or \\\n",
    "           ((first, second) == (1, 2)) or \\\n",
    "           ((second, first) == (1, 2))\n",
    "\n",
    "\n",
    "def can_move_col(array):\n",
    "    \"\"\"Check whether an array can be merged.\"\"\"\n",
    "    for i in range(3):\n",
    "        first, second = array[i], array[i + 1]\n",
    "        if movable_condition(first, second):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def allowed_moves(state):\n",
    "    \"\"\"Find allowed moves for a certain game state\"\"\"\n",
    "    allowed_actions = []\n",
    "    # Check whether the agent can swipe up\n",
    "    if np.any([can_move_col(col) for col in state.T]):\n",
    "        allowed_actions.append('w')\n",
    "    # Check whether the agent can swipe down\n",
    "    if np.any([can_move_col(col[::-1]) for col in state.T]):\n",
    "        allowed_actions.append('s')\n",
    "    # Check whether the agent can swipe left\n",
    "    if np.any([can_move_col(row) for row in state]):\n",
    "        allowed_actions.append('a')\n",
    "    # Check whether the agent can swipe right\n",
    "    if np.any([can_move_col(row[::-1]) for row in state]):\n",
    "        allowed_actions.append('d')\n",
    "    return allowed_actions\n",
    "\n",
    "\n",
    "def try_move_col(array):\n",
    "    \"\"\"Return the next state for an array\"\"\"\n",
    "    new_array = array.copy()\n",
    "    for i in range(3):\n",
    "        first, second = array[i], array[i + 1]\n",
    "        if movable_condition(first, second):\n",
    "            new_array[i] = first + second\n",
    "            new_array[i + 1:] = np.append(new_array[i + 2:], 0)\n",
    "            return new_array\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "\n",
    "def get_reward(current_state, next_state):\n",
    "    \"\"\"Given the current state and the next state, return the reward for the transition action.\"\"\"\n",
    "    reward = 0\n",
    "    # maximum number gets larger\n",
    "    reward += (np.max(next_state) - np.max(current_state))\n",
    "    # more merge\n",
    "    reward += (get_score(next_state) - get_score(current_state))\n",
    "    return reward\n",
    "\n",
    "\n",
    "def get_score(state):\n",
    "    \"\"\"Get score given a game state\"\"\"\n",
    "    all_power = [3**(np.log2(num/3)+1) for row in state for num in row if num > 3]\n",
    "    return np.sum(all_power)\n",
    "\n",
    "\n",
    "def try_move(current_state, action):\n",
    "    \"\"\"Given the state and the chosen action, return the next state\"\"\"\n",
    "    next_state = current_state.copy()\n",
    "    allowed_actions = allowed_moves(current_state)\n",
    "    if action not in allowed_actions:\n",
    "        print(f'Can not move {action}')\n",
    "        return current_state\n",
    "\n",
    "    # Swipe up\n",
    "    if action == 'w':\n",
    "        for i, col in enumerate(current_state.T):\n",
    "            if can_move_col(col):\n",
    "                next_state.T[i] = try_move_col(col)\n",
    "    # Swipe down\n",
    "    elif action == 's':\n",
    "        for i, col in enumerate(current_state.T):\n",
    "            if can_move_col(col[::-1]):\n",
    "                new_array = try_move_col(col[::-1])\n",
    "                next_state.T[i] = new_array[::-1]\n",
    "    # Swipe left\n",
    "    elif action == 'a':\n",
    "        for i, col in enumerate(current_state):\n",
    "            if can_move_col(col):\n",
    "                next_state[i] = try_move_col(col)\n",
    "    # Swipe right\n",
    "    elif action == 'd':\n",
    "        for i, col in enumerate(current_state):\n",
    "            if can_move_col(col[::-1]):\n",
    "                new_array = try_move_col(col[::-1])\n",
    "                next_state[i] = new_array[::-1]\n",
    "\n",
    "    elif action == 'stop':\n",
    "        return current_state, get_score(current_state)\n",
    "\n",
    "    reward = get_reward(current_state, next_state)\n",
    "\n",
    "    return next_state, reward\n",
    "\n",
    "\n",
    "class Threes:\n",
    "    \"\"\"\n",
    "    This is a simulated environment of game Threes.\n",
    "    Swipe direction: {left: 'a', right: 'd', up: 'w', down: 's'}.\n",
    "    There are two levels for this game: ['hard', 'easy'], in which default level is 'hard'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, level='hard'):\n",
    "        \"\"\"Initialize the game\"\"\"\n",
    "        self.state = np.zeros((4, 4))\n",
    "        x, y = np.random.choice(4, 2)\n",
    "        self.state[x, y] = np.random.choice([1, 2])\n",
    "        self.score = get_score(self.state)\n",
    "        self.level = level\n",
    "\n",
    "    def playable(self):\n",
    "        \"\"\"Check whether the game is still playable.\"\"\"\n",
    "        if len(allowed_moves(self.state)) != 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def gen_new_tile(self):\n",
    "        \"\"\"Generate a new tile after each move.\"\"\"\n",
    "\n",
    "        # Basic list of numbers that can be selected\n",
    "        choice_list = [1, 2, 3]\n",
    "\n",
    "        # More number can be selected when the maximum number on the grid gets larger\n",
    "        if np.max(self.state) % 3 == 0:\n",
    "            max_power = np.int(np.log2(np.max(self.state) / 3))\n",
    "            choice_list += [3 * 2 ** i for i in range(max_power + 1)]\n",
    "\n",
    "        # Generate the probabilities for each candidate\n",
    "        if self.level == 'hard':\n",
    "            norm_prob = [1 / len(choice_list)] * len(choice_list)\n",
    "        else:\n",
    "            prob = [i + 1 for i in range(len(choice_list))][::-1]\n",
    "            norm_prob = [num / sum(prob) for num in prob]\n",
    "\n",
    "            # return next number\n",
    "        return np.random.choice(choice_list, p=norm_prob)\n",
    "\n",
    "    def make_move(self, action):\n",
    "        \"\"\"Given the action, the game goes to the next state\"\"\"\n",
    "        if action == 'stop':\n",
    "            self.score = get_score(self.state)\n",
    "            return self.state, self.score\n",
    "\n",
    "        self.state = try_move(self.state, action)[0]\n",
    "\n",
    "        # generate new tile for the current state\n",
    "        try:\n",
    "            new_tile = self.gen_new_tile()\n",
    "        except:\n",
    "            return\n",
    "        loc_0 = np.argwhere(self.state == 0)\n",
    "        x, y = loc_0[np.random.choice(len(loc_0))]\n",
    "\n",
    "        # Update the game state and scores\n",
    "        self.state[x, y] = new_tile\n",
    "        self.score = get_score(self.state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T02:36:34.507087Z",
     "start_time": "2020-06-26T02:36:34.485888Z"
    }
   },
   "outputs": [],
   "source": [
    "def hashable(state):\n",
    "    \"\"\"Switch state matrix to string matrix, so as to make it hashable.\"\"\"\n",
    "    return ', '.join([str(int(i)) for row in state for i in row])\n",
    "\n",
    "\n",
    "def select_best_move_(game):\n",
    "    \"\"\"Selects best move that can get the maximum reward for the next state\"\"\"\n",
    "    possible_next_actions = allowed_moves(game.state)\n",
    "    state_action_score = [(move, try_move(game.state, move)[1])\n",
    "                          for move in possible_next_actions]\n",
    "    max_score = max(state_action_score, key=lambda item: item[1])[1]\n",
    "    max_move_list = [move for move, score in state_action_score if score == max_score]\n",
    "    best_next_move = np.random.choice(max_move_list)\n",
    "    return best_next_move\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    This is an agent to play game \"Threes\". There are two main mode to play the game. One is human mode and the other\n",
    "    is computer mode(demo game). For the computer mode, there are currently three methods to play the game: [\n",
    "    'random', 'max', 'q-learning'] The functions here are inspired by\n",
    "    \"https://github.com/brianspiering/rl-course/blob/master/labs/lab_4_tic_tac_toe/lab_4_tic_tac_toe.ipynb\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, threes, epsilon=0.1, alpha=1.0):\n",
    "        \"\"\"Initial the Agent.\"\"\"\n",
    "        self.V = dict()\n",
    "        self.NewGame = threes\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def state_value(self, game_state, action):\n",
    "        \"\"\"Look up state value. If never seen state, then assume 0.\"\"\"\n",
    "        return self.V.get((hashable(game_state), action), 0.0)\n",
    "\n",
    "    def state_values(self, game_state, actions):\n",
    "        \"\"\"Return a dictionary of state-value pair. It is for finding the action that can maximize the q value \"\"\"\n",
    "        return dict(((hashable(game_state), action), self.state_value(game_state, action)) for action in actions)\n",
    "\n",
    "    def learn_game(self, n_episodes=1000):\n",
    "        \"\"\"Let's learn through complete experience to get that reward.\"\"\"\n",
    "        for e in range(1, n_episodes + 1):\n",
    "            game = self.NewGame()\n",
    "            while game.playable():\n",
    "                action, reward = self.learn_from_move(game)\n",
    "            self.V[(hashable(game.state), action)] = reward\n",
    "\n",
    "    def learn_from_move(self, game):\n",
    "        \"\"\"The heart of Q-learning.\"\"\"\n",
    "\n",
    "        current_state = game.state\n",
    "        # Select next action with epsilon-greedy method\n",
    "        selected_move = self.learn_select_move(game)\n",
    "\n",
    "        # Next state s(t+1) and reward r\n",
    "        next_state, reward = try_move(current_state, selected_move)\n",
    "\n",
    "        # Current state Q value Q(s, a)\n",
    "        old_value = self.state_value(current_state, selected_move)\n",
    "\n",
    "        # best action a* for the next state with the largest q value Q(st+1, a*)\n",
    "        next_max_V, next_max_move = self.select_best_move(game, next_state)\n",
    "\n",
    "        # Q-learning that updates the q-value\n",
    "        self.V[(hashable(current_state), selected_move)] = (1 - self.alpha) * old_value + self.alpha * (\n",
    "                    reward + next_max_V)\n",
    "\n",
    "        game.make_move(selected_move)\n",
    "        return selected_move, reward\n",
    "\n",
    "    def learn_select_move(self, game):\n",
    "        \"\"\"Exploration and exploitation\"\"\"\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            selected_action = np.random.choice(allowed_moves(game.state))\n",
    "        else:\n",
    "            selected_action = self.select_best_move(game, game.state)[1]\n",
    "        return selected_action\n",
    "\n",
    "    def select_best_move(self, game, game_state):\n",
    "        \"\"\"Selects best move for given state(Greedy)\"\"\"\n",
    "        state_action_values = self.state_values(game_state, allowed_moves(game_state))\n",
    "        max_V = max(state_action_values.values())\n",
    "        max_move = np.random.choice([state_action[1] for state_action, v in state_action_values.items() if v == max_V])\n",
    "        return max_V, max_move\n",
    "\n",
    "    def demo_game(self, level='hard', mode='random'):\n",
    "        \"\"\"Agent plays with different policies (random/max/q-learning)\"\"\"\n",
    "        game = self.NewGame()\n",
    "        game.level = level\n",
    "        print(game.state)\n",
    "        while game.playable():\n",
    "            time.sleep(0.5)\n",
    "            if mode == 'random':\n",
    "                next_action = np.random.choice(allowed_moves(game.state))\n",
    "            elif mode == 'max':\n",
    "                next_action = select_best_move_(game)\n",
    "            elif mode == 'q-learning':\n",
    "                next_action = self.select_best_move(game, game.state)[1]\n",
    "            else:\n",
    "                return \"No such mode\"\n",
    "            \n",
    "            print(f'Action: {next_action}')\n",
    "            game.make_move(next_action)\n",
    "            print(game.state)\n",
    "        return game.score\n",
    "\n",
    "    def human_mode(self):\n",
    "        \"\"\"Interactive mode\"\"\"\n",
    "        game = self.NewGame()\n",
    "        level = input('level: easy or hard? \\r')\n",
    "        game.level = level\n",
    "        print(game.state)\n",
    "        while game.playable():\n",
    "            human_allowed_moves = allowed_moves(game.state) + ['stop']\n",
    "            human_move = input(f'You can input {human_allowed_moves} \\r')\n",
    "            if human_move == 'stop':\n",
    "                return f'Game over! Your score is {game.score}'\n",
    "            game.make_move(human_move)\n",
    "            print(game.state)\n",
    "        return f'Game over! Your score is {game.score}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T02:37:36.397370Z",
     "start_time": "2020-06-26T02:36:40.132988Z"
    }
   },
   "outputs": [],
   "source": [
    "human_game = Agent(Threes)\n",
    "\n",
    "print(\"Demo - Human Mode:\")\n",
    "human_game.human_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T02:37:59.398271Z",
     "start_time": "2020-06-26T02:37:45.250494Z"
    }
   },
   "outputs": [],
   "source": [
    "random_game = Agent(Threes)\n",
    "\n",
    "print(\"Demo - Random Mode:\")\n",
    "random_game.demo_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T02:38:42.957933Z",
     "start_time": "2020-06-26T02:38:22.073391Z"
    }
   },
   "outputs": [],
   "source": [
    "greedy_game = Agent(Threes)\n",
    "\n",
    "print(\"Demo - Greedy Mode:\")\n",
    "random_game.demo_game(mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T02:34:58.984Z"
    }
   },
   "outputs": [],
   "source": [
    "human_scores = [135, 2259, 4104, 693, 459, 10809, 20781, 2043, 15120, 486]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T02:34:58.986Z"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(human_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T02:34:58.990Z"
    }
   },
   "outputs": [],
   "source": [
    "np.max(human_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T02:34:58.992Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T02:34:58.994Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_demo_game_stats(agent, n_games=100, level='Hard', mode='random'):\n",
    "    \"\"\"print the result(mean score and max score) of playing demo game\"\"\"\n",
    "    results = [agent.demo_game(level=level, mode=mode) for _ in range(n_games)]\n",
    "    mean_score, max_score = np.mean(results), np.max(results)\n",
    "    print(f\"mean score: {mean_score} | max score: {max_score} \", end='\\r')\n",
    "    return mean_score, max_score\n",
    "\n",
    "\n",
    "def train_qlearning(agent, n_games=100, n_episodes=100, n_training_blocks=10, level='Hard'):\n",
    "    \"\"\"Given agent, do more training. Return (hopefully) improved agent.\"\"\"\n",
    "    for n_training_block in range(1, n_training_blocks + 1):\n",
    "        agent.learn_game(n_episodes)\n",
    "        print(f\"After {n_episodes * n_training_block:,} learning games:\")\n",
    "        mean_score, max_score = print_demo_game_stats(agent, n_games=n_games, level=level, mode='q-learning')\n",
    "    return agent, mean_score, max_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T02:34:58.995Z"
    }
   },
   "outputs": [],
   "source": [
    "random_scores_mean = []\n",
    "random_scores_max = []\n",
    "for _ in range(135):\n",
    "    random_game = Agent(Threes)\n",
    "    mean_score_random, max_score_random = print_demo_game_stats(random_game, n_games=100, level='Hard', mode='random')\n",
    "    print(f'{_:04d} / 134', end='\\r')\n",
    "    random_scores_mean.append(mean_score_random)\n",
    "    random_scores_max.append(max_score_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T02:34:58.996Z"
    }
   },
   "outputs": [],
   "source": [
    "random_df = pd.DataFrame([random_scores_mean, random_scores_max]).T\n",
    "random_df.columns = ['mean_score', 'max_score']\n",
    "random_df.sort_values(mean_score, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T02:34:58.998Z"
    }
   },
   "outputs": [],
   "source": [
    "random_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T02:34:59.001Z"
    }
   },
   "outputs": [],
   "source": [
    "greedy_scores_mean = []\n",
    "greedy_scores_max = []\n",
    "for _ in range(135):\n",
    "    greedy_game = Agent(Threes)\n",
    "    mean_score_greedy, max_score_greedy = print_demo_game_stats(greedy_game, n_games=100, level='Hard', mode='max')\n",
    "    print(f'{_:04d} / 134', end='\\r')\n",
    "    greedy_scores_mean.append(np.mean(mean_score_greedy))\n",
    "    greedy_scores_max.append(np.max(max_score_greedy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T02:34:59.002Z"
    }
   },
   "outputs": [],
   "source": [
    "greedy_df = pd.DataFrame([greedy_scores_mean, greedy_scores_max]).T\n",
    "greedy_df.columns = ['mean_score', 'max_score']\n",
    "greedy_df.sort_values('mean_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T02:34:59.004Z"
    }
   },
   "outputs": [],
   "source": [
    "greedy_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T02:34:59.005Z"
    }
   },
   "outputs": [],
   "source": [
    "random_df['mode'] = 'random'\n",
    "greedy_df['mode'] = 'greedy'\n",
    "com_df = pd.concat([random_df, greedy_df])\n",
    "com_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "Env.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
